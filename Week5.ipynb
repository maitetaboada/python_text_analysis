{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and dataframes\n",
    "\n",
    "Pandas is a python library to work with and store data in a spreadsheet-like format. Pandas allows us to store big datasets of numbers such as type and token counts of hundreds of texts. This notebook introduces some of the basic concepts and functions from Pandas that we'll need, but there are also great introductions out there:\n",
    "\n",
    "* https://www.w3schools.com/python/pandas/pandas_intro.asp\n",
    "* https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/\n",
    "* https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "* http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/\n",
    "\n",
    "We will work with two basic Pandas data structures:\n",
    "\n",
    "* Series\n",
    "* Dataframes\n",
    "\n",
    "A series is very similar to a list or a dictionary. For instance, we have seen lists of words (tokens) extracted from a document and dictionaries of words and their frequencies:\n",
    "\n",
    "* List: `['If', 'you', 'use', '``', 'bad', \"''\", 'to', 'mean', '``', 'good', \"''\", ',', 'then', ...]`\n",
    "* Dictionary: `{',': 31, 'the': 24, '.': 21, 'a': 14, 'and': 14, 'to': 13, \"'s\": 6, 'The': 6, 'I': 6, 'is': 5, ...}`\n",
    "\n",
    "In a series, you'll get that dictionary as a vertical list: \n",
    "\n",
    "```\n",
    ",     31\n",
    "the   24\n",
    ".     21\n",
    "a     14\n",
    "and   14\n",
    "```\n",
    "\n",
    "Then, with Pandas, you can turn those series into dataframes. Dataframes are like spreadsheets. So, from the dictionary above, we can create a dataframe that looks sort of like this:\n",
    "\n",
    "| Token       | Count    |\n",
    "| ----------- | -------- |\n",
    "| ,           | 31       |\n",
    "| the         | 24       |\n",
    "| .           | 21       |\n",
    "| a           | 14       |\n",
    "| and         | 14       |\n",
    "\n",
    "\n",
    "We'll first start with the same process we've done before, where we had files and their token, type, sentence, and lexical diversity count. We'll convert those lists and dictionaries into series and then put them into a dataframe. Finally, and to save the information, we'll use a Pandas function to save that dataframe into a csv file. A comma-separated value file is an easy way to store table information in a text-only format.  \n",
    "\n",
    "We start by importing all the packages we need. Note that we import Pandas as \"pd\". That allows us to type the Pandas functions with a shorthand, \"pd\", rather than the full \"pandas\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import FreqDist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "We'll reuse the functions from Week 4, with some modifications. The first one, `get_text_info()` now only calculates tokens and types. We'll use the pandas dataframe to calculate the lexical diversity.\n",
    "\n",
    "The second function is the same. It reads all the files in the directory and calculates the information from `get_text_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_info(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK to calculate: tokens, types, lexical diversity\n",
    "    \n",
    "    Args:\n",
    "        text (str): a string containing the file or text\n",
    "        \n",
    "    Returns: \n",
    "        dict: a dictionary containing tokens, types, and lexical diversity\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    n_tokens = len(tokens)\n",
    "    n_types = len(set(tokens))\n",
    "    return {\n",
    "            'tokens': n_tokens,\n",
    "            'types': n_types,\n",
    "        }\n",
    "\n",
    "\n",
    "def process_dir(path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a directory. Processes them using the 'get_text_info' function\n",
    "    \n",
    "    Args: \n",
    "        path (str): path to the directory where the files are\n",
    "        \n",
    "    Returns:\n",
    "        dict: a dictionary with file names as keys and the tokens, types, lexical diversity, as values\n",
    "    \n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):    \n",
    "            file_path = os.path.join(path, filename)      \n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                file_info[filename] = get_text_info(text)\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the functions\n",
    "We call the function `process_dir()`, which in turns calls `get_text_info()` and returns a dictionary with the name of the file as key, and n_tokens and n_types for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path. This directory should have more than 1 file\n",
    "path = './data'\n",
    "\n",
    "files_in_dir_info = process_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_dir_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries to pandas dataframe\n",
    "\n",
    "Now, instead of printing the information in `files_in_dir_info`, we convert that information, which is stored as a dictionary, into a dataframe, essentially a table. The information looks like this (see the output above):\n",
    "\n",
    "```\n",
    "    {'Ghostbusters.txt': { 'tokens': 29349, \n",
    "                           'types': 4862},\n",
    "     'middlemarch.txt':  { 'tokens': 374039, \n",
    "                           'types': 20420},\n",
    "     'noise.txt':        { 'tokens': 114158, \n",
    "                           'types': 8603}}\n",
    "```\n",
    "\n",
    "So you see how it's already structured like a table. You can think of the names of the files as headings, and then each file has a row for 'tokens' and 'types'. \n",
    "\n",
    "We use the `DataFrame.from_dict()` function in pandas, giving it the dictionary, so it transforms it into a dataframe. The `orient='index'` flag makes the key in the dictionary the name of the row. So the file names are the rows. \n",
    "\n",
    "You can just print the contents of the dataframe calling the variable `df`. Note the difference in the output when you use `print(df)`. It's just less pretty, because it's text-only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(files_in_dir_info, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on the df\n",
    "\n",
    "Now you can also do operations on this dataframe. For instance, you can calculate the lexical diversity (types/tokens) and add a new column with that information. \n",
    "\n",
    "The first part of the cell, `df['lex_div']` creates a new column, \"lex_div\" in the dataframe. The square brackets are indicating that we are dealing with a part of the df. Then, we use information from other columns in the df, namely 'types' and 'tokens', to populate the 'lex_div' column, one row at a time.\n",
    "\n",
    "This is why I didn't do this calculation as I was reading the files, in the `get_text_info()` function. I can just do it on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lex_div'] = df['types']/df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations on the df\n",
    "\n",
    "You can calculate things from the dataframe, like the average number of tokens or sentences for the entire corpus. These calculations don't change the dataframe; you simply get information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"tokens\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"types\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"lex_div\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv\n",
    "\n",
    "One of the most useful things about pandas is that you can save the information to a csv file directly, using the columns and rows you already have. You can also read in a csv file and convert it to a dataframe. We are going to save all the information from the original file into a \"corpus_info.csv\" file, with the index (the first row) called 'file'.\n",
    "\n",
    "After you run the first cell below, go to your Jupyter directory and open the csv file. Inspect the contents. \n",
    "\n",
    "Then, the second cell reads in that information into a new dataframe, `df_new`. This is not very useful in this notebook, as you already have the original df. But it just shows you how to read in an existing csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to a csv file\n",
    "df.to_csv(\"./data/corpus_info.csv\", index=True, index_label='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in that csv file into a new variable\n",
    "df_new = pd.read_csv(\"./data/corpus_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few useful things you can do on a dataframe\n",
    "\n",
    "There are all kinds of good things you can do with a dataframe. For instance, you can sort it by the values in one of the columns. The first two cells sort the file name in alphabetical order, regular and reverse. The next sorts by number of tokens. \n",
    "\n",
    "You can also drop one of the columns, or add an empty column to add in later. \n",
    "\n",
    "The `head()` method gives you the first 5 rows. You can also give it a number to get the first n rows. `df.head(2)` gives you the first 2 rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by file in alphabetical order\n",
    "df_new = df_new.sort_values('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by file in descending alphabetical order\n",
    "df_new = df_new.sort_values('file', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by tokens in ascending order\n",
    "df_new = df_new.sort_values('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the types column\n",
    "df_new = df_new.drop(['types'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an empty column\n",
    "df_new['new_column'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the first 2 rows\n",
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have learned to use the pandas library to create dataframes, structured tables of information that we can manipulate.\n",
    "\n",
    "We have also learned to write a dataframe to a csv file, and to read a csv file into a dataframe. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
