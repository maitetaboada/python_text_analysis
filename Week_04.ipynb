{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions and n-grams. Functions\n",
    "\n",
    "**Regular expressions** are powerful methods to do the same thing over and over again. When you do a search and replace in something like Microsoft Word, you are using regular expressions. \n",
    "\n",
    "We'll be using the python `re` module, for regular expressions. You can follow along on a [python tutorial about the re module](https://realpython.com/regex-python/). \n",
    "\n",
    "But, first, we'll start with a review of **functions**. \n",
    "\n",
    "We'll also learn **how to structure notebooks** (and code in general) a bit more neatly. We'll start by importing all the modules we need, at the beginning of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "We have been using functions already but we never really went in depth on what they are. A function is really just a way to do some computation and receive a result from that computation. To do this the function needs an input that we provide to it. So the three parts of any function is input, computation, and output. Take for example the `open()` function. We pass it a path to a file and it returns us an opened file. We don't get to see the computation since it is hidden and we don't ask for a print output. \n",
    "\n",
    "Python comes with many predefined functions. However, sometimes we need to create our own functions so that we can easily reuse code.\n",
    "\n",
    "To create a user-defined function we start by defining the function name and input what it takes using `def`. Then we put our code after the defining line. \n",
    "\n",
    "Note the colon and new line. The contents of the function are always indented below the colon. \n",
    "\n",
    "You can also optionally include a statement outlining what the function does. This is like a comment, except it has a slightly different format. Note the 3 quotation marks and that the text appears in a different colour. This is an instruction to python to store that information as special information. You can then retrieve that information with a `help(function_name)` statement.\n",
    "\n",
    "If we want our function to return some answer or result then we need to include a `return` commmand at the end of our function.\n",
    "\n",
    "In the next code block I will create a function for computing lexical diversity. To review how functions work here is a good tutorial: https://www.w3schools.com/python/python_functions.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define our function and its input\n",
    "def lexDiv(types, tokens):\n",
    "    # help or description for the function. Note the format\n",
    "    \"\"\"\n",
    "    Calculates lexical diversity\n",
    "\n",
    "    Args:\n",
    "        2 integers or 2 variables: length of types and length of tokens\n",
    "\n",
    "    Returns:\n",
    "        A float, the lexical diversity\n",
    "    \"\"\"\n",
    "    #  we make a computation, avoiding division by 0\n",
    "    diversity = types/tokens if tokens > 0 else 0\n",
    "    # lastly we return the result\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once defined, we can use, or 'call' the function\n",
    "# here, we call it with 2 fixed numbers. Usually, you'd give it 2 variables (types and tokens)\n",
    "print(lexDiv(4328, 12094))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the description or help on this function\n",
    "help(lexDiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this function, the way it's defined, has 2 arguments: types and tokens. If you try to call the function with only 1 argument, or with 3, you'll get an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lexDiv(4328))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lexDiv(4328, 12094, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function, reading multiple files, and using the function on those files\n",
    "We are now going to put together everything we've learned so far. We'll define a function to use NLTK to calculate types and tokens, create another function to get and process all the files in a directory, and then call the function. This is a more efficient way of doing what we did for Week 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get tokens, types, and lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function. We give it an easy to remember name\n",
    "# and the argument is a variable that contains a string, 'text'\n",
    "def get_text_info(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK to calculate: tokens, types, lexical diversity\n",
    "    \n",
    "    Args:\n",
    "        text (str): a string containing the file or text\n",
    "        \n",
    "    Returns: \n",
    "        dict: a dictionary containing tokens, types, and lexical diversity\n",
    "    \"\"\"\n",
    "    # call the NLTK function to tokenize and store the results in 'tokens'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # get the length of the variable 'tokens'\n",
    "    n_tokens = len(tokens)\n",
    "    # get the length of the types\n",
    "    n_types = len(set(tokens))\n",
    "    # calculate the lexical diversity\n",
    "    # we can do it directly here, or call the function we created above\n",
    "    lexical_diversity = n_types / n_tokens if n_tokens > 0 else 0\n",
    "   # lexical_diversity = lexDiv(n_types, n_tokens)\n",
    "    # we also need to tell the function what information to return\n",
    "    # here, we create a dictionary to store it all\n",
    "    return {\n",
    "            'tokens': n_tokens,\n",
    "            'types': n_types,\n",
    "            'lexical_diversity': lexical_diversity\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read and process all the files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def process_dir(path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a directory. Processes them using the 'get_text_info' function\n",
    "    \n",
    "    Args: \n",
    "        path (str): path to the directory where the files are\n",
    "        \n",
    "    Returns:\n",
    "        dict: a dictionary with file names as keys and the tokens, types, lexical diversity, as values\n",
    "    \n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "\n",
    "    # loop through all the files in the directory \"data\"\n",
    "    for filename in os.listdir(path):\n",
    "        # check only for .txt files\n",
    "        if filename.endswith(\".txt\"):    \n",
    "            # get all the filenames with a .txt extension\n",
    "            file_path = os.path.join(path, filename)      \n",
    "            # open one file at a time, to read it, and with utf encoding\n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                # store the contents of the file into the variable \"text\"\n",
    "                text = f.read()\n",
    "                # call the function on each file\n",
    "                file_info[filename] = get_text_info(text)\n",
    "    # return the info\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the functions\n",
    "Now we call the `process_dir()` function, which calls the `get_text_info()` function. This is a cleaner, more modular way of writing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path. This directory should have more than 1 file\n",
    "path = './data'\n",
    "\n",
    "files_in_dir_info = process_dir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output\n",
    "You can use the `print()` function to check the output. You can also modify this `for()` loop to save this information to a csv file. Note that, because `files_in_dir_info` is a dictionary, we need to go through its `items`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, info in files_in_dir_info.items():\n",
    "    print(f\"File: {file}\")\n",
    "    print(f\"Tokens: {info['tokens']}\")\n",
    "    print(f\"Types: {info['types']}\")\n",
    "    print(f\"Lexical diversity: {info['lexical_diversity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "\n",
    "You often get data that needs a bit of extra cleaning. Regular expressions are a simple yet powerful way to do that. \n",
    "\n",
    "Note that we already imported the `re` module, above. Now we get to use one of its functions, `search()`. As with all functions that come from a module in python, you use it by typing the name of the module before the function: `re.search()`. \n",
    "\n",
    "`re.search()` takes two arguments: \n",
    "1. the pattern you are searching for\n",
    "2. the place where you are searching for it (usually a string)\n",
    "\n",
    "`re.search('e', 'beekeeper)` finds the first instance of the letter 'e' in the word 'beekeeper'. Try it below. \n",
    "\n",
    "But, if you want to find not just the first, but all of the instances, then you use `re.findall()`. Try it below as well.\n",
    "\n",
    "Finally, you can use `re.sub()` to replace text that matches a certain pattern. `re.sub()` takes 3 arguments: the pattern, the thing to replace it with, and the string. Try it below to replace 'the' with 'a'.\n",
    "\n",
    "There are a few useful conventions in regular expressions:\n",
    "\n",
    "* `[]` matches one of the things inside\n",
    "    * `[Tt]` matches either upper or lower case 't'   \n",
    "* `[-]` matches a range\n",
    "    * `[0-9]` matches a single instance of all the numbers from 0 to 9\n",
    "    * `[a-z]` matches all lowercase letters\n",
    "    * `[A-Z]` matches all uppercase letters\n",
    "*  `[*]` matches 0 or more of the previous characters\n",
    "    * `[o*h!]` matches: `h!, oh!, ooh!, oooh!, ooooh!`, etc\n",
    "*  `[+]` matches 1 or more of the previous characters\n",
    "    * `[o+h!]` matches: `oh!, ooh!, oooh!, ooooh!`, etc\n",
    "* `[.]` ignores the previous character\n",
    "    * `[beg.n]` matches: `begin, began, begun`, but also `begon` and `beg3n`. This is useful to identify patterns such as `sh!t`.\n",
    "* Special characters, such as `.`, `,`, or `\\`, need to be preceded by a backslash, so that the regular expression knows that you mean literally the actual punctuation mark, not its use as a regular expression convention.\n",
    "    * `[\\.]` means \"find the first instance of a period\"\n",
    "* `[^]` matches the beginning of the line; `[$]` matches the end of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('e', 'beekeeper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('e', 'beekeeper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The sentence contains the word the spelled in upper and lower case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('[Tt]he', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sentence = re.sub(r'[Tt]he', 'a', sentence)\n",
    "print(a_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up a file\n",
    "The most useful aspect of regular expressions is that you can use them to get rid of stuff you don't need in a file or series of files. Let's say we just want to analyze the language of a script. Then, we want to remove all characters' names and stage directions, as we are likely only interested in the dialogue. \n",
    "\n",
    "We are going to take the Ghostbusters file we downloaded a while back (or any other script from [Scificripts](https://scifiscripts.com/)) and we'll clean it up. \n",
    "\n",
    "First, we read it in into a variable and print it on the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"./data/Ghostbusters.txt\"\n",
    "\n",
    "with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
    "    ghostbusters = f.read()\n",
    "    \n",
    "print(ghostbusters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that most of the stuff we want to get rid of is in all caps on its own line:\n",
    "\n",
    "* FADE IN\n",
    "* EXT. NEW YORK PUBLIC LIBRARY -- DAY\n",
    "* LIBRARIAN\n",
    "\n",
    "We'll write a regular expression using `re.sub()` to find all instances of entire lines that are only in upper case and replace them with nothing. Note that only uppercase lines also have spaces, periods and hyphens.\n",
    "\n",
    "* The first argument is: `^[A-Z\\s\\.\\,\\-]+$\\n?`\n",
    "    * `^` indicates the beginning of the line\n",
    "    * `A-Z` means any uppercase character\n",
    "    * `\\s` means space\n",
    "    * `\\.` means period\n",
    "    * `\\,` means comma\n",
    "    * `\\-` means hyphen\n",
    "    * `+` means 1 or more instances of an uppercase character\n",
    "    * `$` means the end of the line\n",
    "    * `\\n` means the end of line character\n",
    "    * `?` means that the `\\n` is optional\n",
    "* The second argument is: `''`\n",
    "    * This means 'replace with nothing', as there are no characters between the quotes\n",
    "* The third argument is the variable to operate on\n",
    "* We also use the flag MULTILINE to make sure we match the entire line, not just a string within the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghostbusters_clean = re.sub(r'^[A-Z\\s\\.\\,\\-]+$\\n?', '', ghostbusters, flags=re.MULTILINE)\n",
    "print(ghostbusters_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are still a couple of things that should be removed:\n",
    "\n",
    "* lines that contain a character name plus parentheses: VENKMAN (V.O.)\n",
    "* text in parentheses: (puzzled)\n",
    "\n",
    "You can write additional regular expressions to deal with those. For the first case, you can just probably modify the regular expression above to include parentheses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you have learned some concepts about writing and using functions:\n",
    "\n",
    "* Structure and arguments for functions\n",
    "* How to write the help part of functions\n",
    "* How to use a function to read and process files in a directory\n",
    "\n",
    "You have also learned about regular expressions:\n",
    "\n",
    "* Main functions in `re`\n",
    "* Some operators for regular expressions\n",
    "* How to use regular expressions to clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
