{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing data. Named entity recognition\n",
    "We have by now learned the basics of working with python and with python data types. We have also learned to process files. We are moving to doing more interesting things with textual data. In this lesson, we will learn about cleaning and normalizing data and how to identify named entities.\n",
    "\n",
    "We will continue to use NLTK, but we will also install another powerful Natural Language Processing package, spaCy. If you haven't, go to the spacy_install.ipynb notebook and follow instructions there. Then come back here to import and use spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: pandas for storing information into a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# we need to import NLTK every time we want to use it\n",
    "import nltk\n",
    "\n",
    "# import the NLTK packages we know we need\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# these 2 packages may already be in your system, but just in case\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import spaCy and the small English language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# this does prettier displays on spaCy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing data\n",
    "Normalizing refers to a set of processes to make data uniform. This is generally useful to count things the correct way and to get to the essence of the words in a text. Think of counting the instances of the word \"the\" in a text. You'll want to make sure that \"the\", \"The\", and \"the?\" all look the same before you count them. Normalization includes:\n",
    "\n",
    "* Converting all words to lowercase\n",
    "* Removing or separating punctuation\n",
    "* Stemming - removing endings and ending up with the _stem_ (endings -> end; went -> went)\n",
    "* Lemmatizing - removing endings and ending up with the _root_ (endings -> end; went -> go)\n",
    "\n",
    "Most NLP packages (NLTK, spaCy) have built-in methods to do this. But it's also good to know how to do it yourself, in case you want to control what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an excerpt from The Peak, https://the-peak.ca/2025/01/sfu-study-calls-for-utility-scale-solar-power-systems-in-canada/\n",
    "text1 = \"In December 2024, Clean Energy Research Group (CERG) published a paper calling for Canada to build “mass utility-scale solar mega projects,” according to an SFU news release. Utility-scale solar “refers to large solar installations designed to feed power directly onto the electric grid.” An electric grid is an “intricate system” that provides electricity “all the way from its generation to the customers that use it for their daily needs.”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a made-up text\n",
    "text2 = \"This gotta be the wéïrdest bit of text that's ne'er gonna be The thing you'll encounter, but I have to give, gave, given, something!\\r, even the weirdest. Just tryna throw everything into a made-up bit that isn't making any sense.<br> And here's a sentence with the irregular plural feet and one with the irregular plural geese.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing without lowercase\n",
    "We will use NLTK to tokenize the texts. You can print the list of tokens, and also print the count of types and tokens. You'll see that 'And' and 'and' count as two different types. But they are really the same word. That's why we lowercase first or lowercase after tokenizing, but before counting. Compare this bit of code and the output to what happens if we lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = nltk.word_tokenize(text1)\n",
    "n_tokens1 = len(tokens1)\n",
    "n_types1 = len(set(tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in set(tokens1):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens1)\n",
    "print(n_types1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but for text2\n",
    "tokens2 = nltk.word_tokenize(text2)\n",
    "n_tokens2 = len(tokens2)\n",
    "n_types2 = len(set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in set(tokens2):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens2)\n",
    "print(n_types2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing after lowercasing\n",
    "Compare the numbers and the output now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using the lower() method in a string\n",
    "tokens1_lower = [w.lower() for w in tokens1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_types1_lower = len(set(tokens1_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens1)\n",
    "print(n_types1)\n",
    "print(n_types1_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for text 2\n",
    "tokens2_lower = [w.lower() for w in tokens2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_types2_lower = len(set(tokens2_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens2)\n",
    "print(n_types2)\n",
    "print(n_types2_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing with NLTK\n",
    "\n",
    "### Stemming\n",
    "Stemmers remove any endings that may be [inflectional suffixes](https://en.wikipedia.org/wiki/Inflection) in English. There are different versions of stemmers, even within NLTK. See the [overview of stemming in NLTK](https://www.nltk.org/howto/stem.html). Here, we'll use the [Porter Stemmer](https://www.nltk.org/_modules/nltk/stem/porter.html), developed by Martin Porter. \n",
    "\n",
    "Look at the output carefully and note where things don't seem to make sense. This is because Porter removes anything that may possibly be an ending, including the '-er' in 'December', because it is sometimes an inflectional ending in words like 'clever'.\n",
    "\n",
    "### Lemmatization\n",
    "Lemmatizers are a bit smarter about inflection and are able to identify roots, even when no suffixes are involved (gave -> give; feet -> foot). We'll use the [WordNet lemmatizer](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet) in NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the stemmer to a variable, 'stemmer'\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# go through the list of tokens (tokens1)\n",
    "# lower the tokens in that list\n",
    "# use list comprehension (with the square brackets)\n",
    " # so that the stemmer can iterate over the list\n",
    "tokens1_st = [stemmer.stem(token.lower()) for token in tokens1]\n",
    "tokens2_st = [stemmer.stem(token.lower()) for token in tokens2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens1_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the lemmatizer to a variable\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# go through the list of tokens and lemmatize\n",
    "tokens1_lm = [lemmatizer.lemmatize(token.lower()) for token in tokens1]\n",
    "tokens2_lm = [lemmatizer.lemmatize(token.lower()) for token in tokens2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options: remove non-ASCII characters, remove HTML tags\n",
    "There are additional things you may want to do to clean and normalize text, including converting everything to ASCII (wéïrdest -> weirdest) and removing HTML tags ('`<br>`', -> '' ). You'll probably want to do that before tokenization, so that the angle brackets in HTML don't get tokenized as punctuation. \n",
    "\n",
    "The [emoji](https://pypi.org/project/emoji/) library can also convert UTF emoji into descriptions (&#x1F917; -> 'hugging face')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing text with spaCy\n",
    "Now that you have seen how to do this with NLTK, the good news is that spaCy will do pretty much everything you need to do to clean and normalize text. It will also give you morphological and syntactic information about it. Go to the `spacy_install.ipynb` notebook for more information on how spaCy works.\n",
    "\n",
    "We call spaCy by using the `nlp` object and passing it the text that we want to process. Then we can query and print the information contained in the `doc` object that spaCy creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the text with spaCy\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 'text' attribute of each of the tokens\n",
    "for token in doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see this in a single line, you can join the strings in the list, with a space between each\n",
    "print(\" \".join([token.text for token in doc1]))\n",
    "print(\"\\r\") # this is just so that the two texts are separated on the screen by an empty line\n",
    "print(\" \".join([token.text for token in doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the lemmas\n",
    "print(\" \".join([token.lemma_ for token in doc1]))\n",
    "print(\"\\r\")\n",
    "print(\" \".join([token.lemma_ for token in doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the part of speech of each word after the word\n",
    "print(\" \".join([f\"{token.text}/{token.pos_}\" for token in doc1]))\n",
    "print(\"\\r\")\n",
    "print(\" \".join([f\"{token.text}/{token.pos_}\" for token in doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the doc object in spaCy contains all kinds of information\n",
    "# including rich morphology for each word\n",
    "for token in doc1:\n",
    "    print(token.text, \"\\t\\t\", token.lemma_, \"\\t\\t\", token.pos_, \"\\t\\t\", token.morph) # the \\t helps show sort of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't know what the abbreviations mean, \n",
    "# you can ask for an explanation\n",
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc also includes syntactic information about heads and dependents\n",
    "# including rich morphology for each word\n",
    "for token in doc1:\n",
    "    print(token.text, \"\\t\\t\", token.pos_, \"\\t\\t\", token.dep_, \"\\t\\t\", token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btw, you can still count tokens and types with spaCy\n",
    "tokens1 = [token.text for token in doc1]\n",
    "types1 = set(tokens1)\n",
    "\n",
    "# you can see those lists\n",
    "print(\"tokens: \", tokens1)\n",
    "print(\"\\r\")\n",
    "print(\"types: \", types1)\n",
    "\n",
    "# and you can print their length\n",
    "print(\"\\r\")\n",
    "print(\"number of tokens: \", len(tokens1))\n",
    "print(\"number of types: \", len(types1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pandas to show the output\n",
    "You can also store the information into a pandas dataframe, which makes it much more readable and easy to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "\n",
    "for token in doc1:\n",
    "    data1.append([token.text, token.pos_, token.dep_, token.head])\n",
    "    \n",
    "df = pd.DataFrame(data1)\n",
    "df.columns = ['Text', 'Tag', 'Dependency', 'Head']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "Named Entity Recognition (NER) is the process of identifying and labelling _named entities_, that is, real world objects, locations, and identifiers such as dates, currency, or quantities. It is very useful if you want to know, for instance, who is mentioned in a text, which countries are involved, or which dates. It is what allows your email or messaging application to identify dates and suggest a calendar entry, as in the image below, from the iPhone Notes app. \n",
    "\n",
    "\n",
    "<a href=\"./img/ner_date.jpeg\" target=\"_blank\">\n",
    "<img src=\"./img/ner_date.jpeg\" width=\"100\" height=\"200\" style=\"border: 1px solid gray; padding: 5px;\"> </a>\n",
    "\n",
    "spaCy has a pretty powerful NER module. It can give you the named entities in a text, with the label for each word that is part of the entity. It also knows the boundaries of the entire entity. So it knows that \"Clean\", \"Energy\", \"Research\", and \"Group\" all have the ORG (for organization) label, but it also knows that the full entity is \"Clean Energy Research Group\". \n",
    "\n",
    "You can list all the entities in a text with the usual for loop. Using the [displacy module](https://spacy.io/usage/visualizers), you can also visualize the boundaries and the types in different colours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each token and its entity label, if it has one\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for doc2, but it doesn't contain entities\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's useful to count and store the named entities in a text\n",
    "\n",
    "# create an empty list\n",
    "named_ents1 = []\n",
    "\n",
    "# go through the entities and append each to the list\n",
    "for ent in doc1.ents:\n",
    "    named_ents1.append((ent.text, ent.label_))\n",
    "    \n",
    "print(named_ents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df for the entities, from the list above \n",
    "df_ents1 = pd.DataFrame(named_ents1)\n",
    "# name the columns\n",
    "df_ents1.columns = ['Entity', 'Label']\n",
    "# print\n",
    "df_ents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the entities\n",
    "displacy.render(doc1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have learned quite a bit! Text normalization includes:\n",
    "\n",
    "* Lowercasing the words\n",
    "* Tokenizing (identifying words, punctuation, anything else)\n",
    "* Stemming\n",
    "* Lemmatizing\n",
    "\n",
    "And we have learned to do this with both NLTK and spaCy.\n",
    "\n",
    "We have also learned about **Named Entity Recognition** and how to extract entities with spaCy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
