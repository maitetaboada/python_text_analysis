{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI\n",
    "\n",
    "Generative Artificial Intelligence refers to a class of methods and algorithms that use Large Language Models to **generate** text. The most common way we experience that generative capacity is with models like ChatGPT, where we give it an input and it generates an answer. \n",
    "\n",
    "There are tons of good explanations out there for these models. Terms that you should understand are:\n",
    "\n",
    "* Artificial intelligence (AI) - an attempt to model human intelligence\n",
    "* Machine learning (ML) - learning to identify patterns in data, usually to identify (predict) new patterns in new data\n",
    "* Neural machine learning, neural models - a class of ML models that take as inspiration the structure of the human brain (neurons and connections)\n",
    "* Deep learning - a class of ML models that use neural models, with several layers of 'neurons', and a depth of several layers\n",
    "* Large language models (LLMs) - models of language based on the principle of sequences: You can predict the next word/token based on the previous word(s).\n",
    "* Generative AI - models that predict the next word based on LLMs\n",
    "\n",
    "We are dealing with language in this course, which is why I talk about predicting the next word or token. Generative AI also works for images, video, code, and other types of data. \n",
    "\n",
    "The basic principle behind most modern Generative AI models is a classic saying from linguist [J.R. Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth) (1890-1960): \"you shall know a word by the company it keeps\". Originally, this meant that you can understand [the meaning of a word](https://ecampusontario.pressbooks.pub/essentialsoflinguistics2/chapter/7-5-lexical-meaning/) by looking at the sentences or context it appears in. In modern NLP and LLMs, it means that we can create a model of language based on [**collocation**](http://glottopedia.org/index.php/Collocation), the way that words occur together.\n",
    "\n",
    "A language model is just the encoding of knowledge about likely sequences and word associations. You can create a language model by obtaining statistics about a large corpus of language. You can also create a large language model (LLM) like the ones behind ChatGPT or Llama using deep learning. \n",
    "\n",
    "To understand this, we are going to work with **ngrams** (also spelled n-grams). Ngrams are sequences of tokens (usually words, but sometimes also punctuation) of size _n_. Terms:\n",
    "\n",
    "* ngram / unigram - one token/word\n",
    "* bigram / 2-gram - sequence of two tokens \n",
    "* trigram / 3-gram - sequence of three tokens\n",
    "* 4-gram - sequence of four tokens\n",
    "* etc.\n",
    "\n",
    "We'll create simple language models from ngrams using NLTK. The intuition about n-grams is that you can predict the next _n_ in a sequence if you know the frequencies of pairs of _n_ items from corpora. To make things simple, let's think of _n_ as 2. And we will assume _n_ is a word. But you can also calculate ngram frequency for characters, sounds, or sentences. \n",
    "\n",
    "Thus, we can figure out what the next word is if we know the previous words are. Let's say that we want to find out the likelihood that the next word in the sequence _I really like_ is _you_. This is, by the way, what Google suggested when I typed _I really like..._ The first link was to a [Carly Rae Jepsen song](https://youtu.be/qV5lzRHrGeg). We can calculate that as:\n",
    "\n",
    "\n",
    "$$ P(you | I, really, like ) $$\n",
    "\n",
    "The way that formula is written is a 4-gram (a sequence of 4 words). This can be difficult to calculate, especially for less frequent combinations of sentences. So, to make this into a bigram probability, we calculate the following, which reads as \"the probability of _you_ given _like_\": \n",
    "\n",
    "$$ P(you | like ) $$\n",
    "\n",
    "The general formula is below. The probability of $w_i$ given the sequence $w_1$ to $w_{i-1}$ is approximately the probability of  $w_i$ given $w_{i-1}$. So, instead of calculating probabilities for a long sequence of words, we do it for a sequence of 2 words at a time.\n",
    "\n",
    "$$ P(w_i | w_1, w_2, w_3, ..., w_{i-1} ) \\approx P(w_i | w_{i-1}) $$\n",
    "\n",
    "Note that above we say \"the probability of _x_ given _y_\". To calculate that, we just count how often any 2 words appear in a large enough corpus. This is what we'll do in this notebook!\n",
    "\n",
    "Credits: [NLTK LM documentation](https://github.com/nltk/nltk/blob/develop/nltk/lm/__init__.py), [N-gram language models](https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk), [N-gram language modelling with NLTK](https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/), [Predicting next word using n-gram model NLTK](https://stackoverflow.com/questions/75565130/predicting-next-word-using-n-gram-model-nltk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements\n",
    "\n",
    "We import everything we need, including bits of NLTK. To train only on \"important\" or content words, we will remove punctuation and [stopwords](https://en.wikipedia.org/wiki/Stop_word). We'll use the [NLTK Reuters corpus](https://www.nltk.org/book/ch02.html#reuters-corpus) to train, so we need to import that too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.corpus import reuters \n",
    "from nltk import FreqDist \n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('reuters') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK ngram functions\n",
    "\n",
    "There are several functions in NLTK that you can use. We start with `nltk.bigrams()`. It takes a list of tokens as input and gives you all the possible bigrams of words. Then we can compute the frequency distribution of those bigrams. \n",
    "\n",
    "But the best function is `everygrams`, which builds as many ngrams as you like from an input. You give it word tokens (but it can also be used with character tokens) and tell it how many types of ngrams to build. In my example, I say `1, 3`, which means: give me: unigrams, bigrams, trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"I really like you.\"\n",
    "\n",
    "sent1_tokens = nltk.word_tokenize(sent1)\n",
    "\n",
    "sent1_bi = nltk.bigrams(sent1_tokens)\n",
    "\n",
    "# compute frequency distribution for all the bigrams in the text\n",
    "sent1_fdist = nltk.FreqDist(sent1_bi)\n",
    "\n",
    "for key, value in sent1_fdist.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, with another sentence\n",
    "# note that \"really really\" now has a frequency of 2\n",
    "\n",
    "sent2 = \"I really really really very much like you.\"\n",
    "\n",
    "sent2_tokens = nltk.word_tokenize(sent2)\n",
    "\n",
    "sent2_bi = nltk.bigrams(sent2_tokens)\n",
    "\n",
    "sent2_fdist = nltk.FreqDist(sent2_bi)\n",
    "\n",
    "for k, v in sent2_fdist.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, everygrams for sent1\n",
    "\n",
    "sent1_every = everygrams(sent1_tokens, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sent1_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build everygrams for sent2\n",
    "# you can also build them of different length (1-2, 1-3, 1-4, etc)\n",
    "# and you can try your own sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating ngram frequencies from Reuters\n",
    "\n",
    "Ngrams are really useful when we have large numbers of them and their frequencies. In this part, we take all the sentences in the Reuters corpus and count their frequencies. Then, we create a `removal_list` with all the things that we want to strip (punctuation and stopwords). Then, we create the lists of unigrams,  of bigrams and trigrams, padding to the left and to the right. Padding just means adding a special \"word\" that indicates the beginning and end of a sentence, so that the first and last words also participate in all possible bigrams. \n",
    "\n",
    "For instance, in _I really like you_, we could have the following bigrams:\n",
    "\n",
    "```\n",
    "I, really\n",
    "really, like\n",
    "like, you\n",
    "```\n",
    "\n",
    "But notice how, unlike the other words, _I_ and _you_ only participate in one bigram. We want to know that that's because they are the beginning and end of the sentence. Padding adds that information, which here I am representing with the html code `<s>` and `</s>`. So then we create the following bigrams:\n",
    "\n",
    "```\n",
    "<s>, I\n",
    "I, really\n",
    "really, like\n",
    "like, you\n",
    "you, </s>\n",
    "```\n",
    "\n",
    "So, we will first create a set of removal words, punctuation and stopwords that we don't want to include the in the lists of ngrams. You can see what it contains below. \n",
    "\n",
    "Next, we import the Reuters sentences and use `everygrams` to create unigrams, bigrams, and trigrams. We remove those that have words in the removal list. \n",
    "\n",
    "After that, `word_salad`is a dictionary with the frequency distribution of those ngrams. \n",
    "\n",
    "Finally, we use `word_salad` to create a sequence of segments that start with a certain prompt. The segments are made up of the prompt, plus the most likely next word. Thus, if the prompt is \"it will\", then we'll get the following:\n",
    "\n",
    "```\n",
    "('it', 'will'), \n",
    "('it', 'will', 'be'), \n",
    "('It', 'will'), \n",
    "('it', 'will', 'pay'), \n",
    "('it', 'will', 'not'), \n",
    "('IT', 'WILL'), \n",
    "('it', 'will', 'continue'), \n",
    "('it', 'will', 'have'), \n",
    "('it', 'will', 'make'), \n",
    "('it', 'will', 'take'), \n",
    "('It', 'will', 'be'), \n",
    "('it', 'will', 'raise'), \n",
    "('it', 'will', 'acquire'), \n",
    "('it', 'will', 'also'), \n",
    "('it', 'will', 'report'), \n",
    "('it', 'will', 'offer'), \n",
    "('it', 'will', 'issue'), \n",
    "('it', 'will', 'receive'), \n",
    "('it', 'will', 'increase'), \n",
    "('it', 'will', 'sell'),\n",
    " etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of things we'll remove (some punctuation, stopwords, and end of line codes)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "my_punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—'\n",
    "removal_list = set(stop_words) | set(my_punctuation) | {'lt','rt', '\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just check what's in that list\n",
    "removal_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data to create ngrams\n",
    "\n",
    "Here, we will use the [Reuters corpus](https://www.nltk.org/book/ch02.html#reuters-corpus). After you import it into `sents`, check its contents. You'll see that it's a list of sentences, and then a list of words in those sentences. \n",
    "\n",
    "NOTE for lab: If you are going to do this with regular text data (text that you read from a file), you need to first tokenize each sentence. You can use code like the following, given that the text you read from the file is called `text_from_file`.\n",
    "\n",
    "`text_tokenized = word_tokenize(text_from_file)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Reuters and inspect it\n",
    "sents = reuters.sents()\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and use a function to clean the text\n",
    "\n",
    "This function **flattens** the list of sentences, which are in turn a list of words, and flattens them to simply a list of words. This is because Reuters provides a list of sentences. For plain text, you'd skip the 'flattened' part of this function.\n",
    "\n",
    "Then, call the function to clean the text and remove all the stopwords, punctuation, etc., that are in the `removal_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_flattened_sentences(sentences):\n",
    "    # flatten the list of sentences into a single list of words\n",
    "    flattened = [word for sent in sentences for word in sent]\n",
    "    \n",
    "    # clean the flattened list by removing stop words and punctuation\n",
    "    cleaned_flattened = [word for word in flattened if word not in removal_list]\n",
    "    \n",
    "    return cleaned_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "cleaned_flat_sentences = clean_flattened_sentences(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 50 cleaned words, just to inspect the list\n",
    "print(cleaned_flat_sentences[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run everygrams and create a dictionary\n",
    "\n",
    "`everygrams()` creates a list of all the unigram, bigram, trigram combinations in the text. Then, we use `FreqDist()` to create a dictionary with the frequency of each of those ngrams. \n",
    "\n",
    "### Warning\n",
    "`one_two_three_ngrams` is a [generator](https://realpython.com/introduction-to-python-generators/) (see what happens when I say `type(one_two_thre_ngrams()`), so you shouldn't try to print it or inspect it before you do the FreqDist. Just run the next two lines one after the other.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ngrams\n",
    "one_two_three_ngrams = everygrams(cleaned_flat_sentences, 1, 3, pad_left=True, pad_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with the frequency of all the ngrams\n",
    "word_salad = FreqDist(one_two_three_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just so that you know this variable is a generator\n",
    "type(one_two_three_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the word salad. It should be a dictionary\n",
    "word_salad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary in reverse order (the result is a list, but that's fine, as we only want to see it)\n",
    "word_salad_ordered = sorted(word_salad.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 20 items\n",
    "word_salad_ordered[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the last 20 items\n",
    "word_salad_ordered[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text from the word salad lists\n",
    "\n",
    "I can give the list of ngrams a prefix, or a 'prompt', and it will give me all the possible things that come after it. This is just an unordered list, but you can also order it by the most frequent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an input \"prompt\"\n",
    "prefix = 'company loss'\n",
    "\n",
    "# Check what's most likely to come next\n",
    "# make sure there are no None values\n",
    "matching_ngrams = [\n",
    "    ng for ng in word_salad \n",
    "    if all(word is not None for word in ng) and ' '.join(ng).lower().startswith(prefix.lower())\n",
    "]\n",
    "\n",
    "print(matching_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a different prompt\n",
    "prefix = 'they said'\n",
    "\n",
    "matching_ngrams = [\n",
    "    ng for ng in word_salad \n",
    "    if all(word is not None for word in ng) and ' '.join(ng).lower().startswith(prefix.lower())\n",
    "]\n",
    "\n",
    "print(matching_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have learned a basic principle of generative AI: to predict the next word in a sequence based on a corpus. \n",
    "\n",
    "Note that ngrams are not actually generative AI. LLMs use vector embeddings instead of word representations (tokens). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
