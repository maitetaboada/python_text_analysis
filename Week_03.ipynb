{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing\n",
    "\n",
    "Most data you'll encounter contains extremely useful information and information that you don't really need or that is actually noise. Cleaning and preprocessing allows you keep the useful information. It also puts the data in formats that will be easy to process by python and other programs.\n",
    "\n",
    "\n",
    "### File Processing\n",
    "\n",
    "To understand file processing and encodings read the file manipulation tutorial which will give an overview of all of the techniques and functions you will be using: [file manipulation](https://realpython.com/read-write-files-python/)\n",
    "\n",
    "Opening a file using the `with open() as` will store the opened file in a data type that we can then use to read the file contents and store the contents to a variable. I will demonstrate how to open a file and save the contents to a string variable. The `open()` function takes two inputs: first the name of the file, and second whether you want to read(\"r\") or write(\"w\") to the file.\n",
    "\n",
    "First let's create our own text file in the data folder. You should have a folder called 'data' from last week, which contains the Ghostbusters script. Go to that directory, and create your own text file there. Using notepad create a file with some text in it and save it inside the data folder. I've done this and called my file 'sample.txt'.\n",
    "\n",
    "If you don't have the 'data' folder, navigate to the directory where this notebook sits (remember you can use `pwd` to find it) and create a new directory, called 'data'. Then, create a text file inside called \"sample.txt\".\n",
    "\n",
    "Write something in that file. \n",
    "\n",
    "Now read through the code block to see how you can open and read this file.\n",
    "\n",
    "To learn more about filepaths read the following chapter up to and including the \"Absolute vs. Relative Paths\": https://automatetheboringstuff.com/3e/chapter10.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./data/sample.txt\"  #relative path\n",
    "\n",
    "# filePath = \"C:/Maite/MOD/notebooks/Ling380/data/sample.txt\" #absolute path\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    data = f.read()\n",
    "    \n",
    "# we can now print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "#filePath = \"./data/sample.txt\"  #relative path\n",
    "\n",
    "filePath = \"C:/Maite/MOD/notebooks/Ling380/data/sample.txt\" #absolute path\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    data = f.read()\n",
    "    \n",
    "# we can now print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "The instructions above helped us open a file that is in plain text format. You can use the same instructions to open the Ghostbusters file from last week, because it's also in plain text format.\n",
    "\n",
    "Most text, however, has more advanced information. This includes things like curly brackets and apostrophes (instead of straight quotes), but also accents, umlauts, emoji and many other special characters that plain text cannot handle. Then you need to use other forms of **encoding,** ways to store information that goes beyond the simple character. Character encoding maps characters to numerical representations. \n",
    "\n",
    "\n",
    "### ASCII\n",
    "\n",
    "[ASCII](https://en.wikipedia.org/wiki/ASCII), which stands for American Standard Code for Information Interchange, is a pioneering character encoding system that has provided a foundation for many modern character encoding systems.\n",
    "\n",
    "ASCII is still widely used, but is very limited in terms of its character range. If your language happens to include characters such as √§ or √∂, you are out of luck with ASCII.\n",
    "\n",
    "### Unicode\n",
    "\n",
    "[Unicode](https://en.wikipedia.org/wiki/Unicode) is a standard for encoding text in most writing systems used across the world, covering nearly 140,000 characters in modern and historic scripts, symbols and emoji.\n",
    "\n",
    "For example, the pizza slice emoji üçï has the Unicode \"code\" `U+1F355`, whereas the corresponding code for a whitespace is `U+0020`.\n",
    "\n",
    "Unicode can be implemented by different character encodings, such as [UTF-8](https://en.wikipedia.org/wiki/UTF-8), which is defined by the Unicode standard. If you don't know the encoding of your text, it is a good bet that it is in UTF-8. \n",
    "\n",
    "Don't think that Unicode is only for foreign accents or emoji. Even in plain English, whenever you change from straight quotes to curly quotes (aka smart quotes), you are using Unicode. Note the difference in the quotes and the apostrophe in the two sentences below.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./img/curly_quotes.png\"\n",
    "         alt=\"quote styles\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Practice\n",
    "\n",
    "Let us know try and see what happens if we read text that is encoded in UTF-8 as plain text. Go to [Radio Canada](https://ici.radio-canada.ca/) and copy the first paragraph of the first article you find there. Save it in a text file in the data directory, let's call the file \"radio-canada.txt\". Now, try to read the file and print it on screen. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./data/radio-canada.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    data = f.read()\n",
    "    \n",
    "# we can now print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same thing, but add the flag `encoding=\"utf-8\"` to the open command. What is the difference in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./data/radio-canada.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    data = f.read()\n",
    "    \n",
    "# we can now print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File formats\n",
    "\n",
    "ASCII and UTF refer to the encoding of the text itself. In addition, files can have different formats, usually represented by their extension (.docx, .csv, .pdf, etc.). The extension tells the operating system which application to use to open the file. \n",
    "\n",
    "For data processing, different extensions (= different file formats) have different properties. The table [from the article by Han (2022)](https://doi.org/10.7551/mitpress/12200.003.0010) is a nice summary of their properties. We will most often deal with plain text formats that are suitable for data exchange, including: .txt, .csv, .xml, and .json.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./img/file_formats.png\"\n",
    "         alt=\"file formats\">\n",
    "    <figcaption>Common file formats, from Han (2022), https://doi.org/10.7551/mitpress/12200.003.0010</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Tip: if you want to see the file extensions on your system, you can change the options in [Windows](https://support.microsoft.com/en-us/windows/common-file-name-extensions-in-windows-da4a4430-8e76-89c5-59f7-1cdbbc75cb01) or [Mac](https://support.apple.com/en-ca/guide/mac-help/mchlp2304/mac) so that you can see them in Explorer/Finder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a file\n",
    "\n",
    "Now that we know about file formats and about reading from files, let's do one last exercise, printing to a file. We'll work with the Ghostbusters file from last week. First, we'll read it into a variable. Then, we'll count the number of words in it (tokens) and we'll write that information to a csv file.\n",
    "\n",
    "This is a very simple way of counting words, and it will count both actual words and punctuation or numbers. We'll learn how to do this more smartly, using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./data/Ghostbusters.txt\"\n",
    "# create a variable to hold the word counter\n",
    "# set it to 0 \n",
    "n_words = 0\n",
    "\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
    "    # read the data from f and store it in the string variable \"ghostbusters\"\n",
    "    ghostbusters = f.read()\n",
    "    \n",
    "    # use the split() function, to split text at spaces\n",
    "    lines = ghostbusters.split()\n",
    "\n",
    "    # create a variable, n_words, and add to it every time the lines counter increases\n",
    "    # note the += This reassigns the value of lines to n_words (=) and increases it (+)\n",
    "    n_words += len(lines)\n",
    "\n",
    "\n",
    "# Print the final value\n",
    "print(n_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv library, to manipulate csv files\n",
    "import csv\n",
    "\n",
    "# create a new file in csv format\n",
    "with open('./data/file_lengths.csv', 'w') as out:\n",
    "    # create file writer object\n",
    "    writer = csv.writer(out)\n",
    "    \n",
    "    # the first row creates headings\n",
    "    writer.writerow(['file', 'length'])\n",
    "    \n",
    "    # write the name of the file and the length in the second row \n",
    "    writer.writerow([filePath, n_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to NLTK\n",
    "\n",
    "\n",
    "NLTK (Natural Language ToolKit) is a light-weight (but still quite powerful) NLP tool. You can learn more about it from the [NLTK website](https://www.nltk.org/) and the [NLTK book](https://www.nltk.org/book/). \n",
    "\n",
    "If you haven't installed NLTK yet, go to the `nltk_install.ipynb` notebook in our [Ling380 repository](https://github.com/maitetaboada/Ling380) and run it. Then come back here. The next cell just imports all the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the text we read in above, which should be in the variable `ghostbusters`. We will **tokenize** that text, that is, convert the string of text into a list of words and punctuation (aka tokens). When you print below, note the format of the list, with tokens in quotes (because they are strings) and separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the word_tokenize function from NLTK\n",
    "tokens = nltk.word_tokenize(ghostbusters)\n",
    "\n",
    "# just check what the variable contains\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say I'm interested in how many tokens this script contains. I can use the built-in python function `len()` to find that out and store it in the variable `n_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(tokens)\n",
    "print(n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the **tokens**, i.e., the words and punctuation in the script. That's going to include repeated instances of each token. There'll be many instances of the **type** \"the\" or \".\". Types refers to the unique tokens. To find how many types or unique tokens the text has, we use the `set()` function and assign the result to the variable `n_types`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_types = len(set(tokens))\n",
    "print(n_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another interesting feature of text, the **lexical diversity**. Lexical diversity just measures how many unique words are used with regard to how many total words there are. This is an interesting measure, for instance, for [children's books](https://readabilityformulas.com/what-are-lexical-density-and-lexical-diversity/). You want to have a relatively low lexical diversity, so that young readers are not overwhelmed by too many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity = n_types / n_tokens\n",
    "print(lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an interesting thing you can do with a file is to create a dictionary of all the tokens in the text and count how many times they appear. NLTK does this with the FreqDist function (frequenty distribution). \n",
    "\n",
    "Note that the output of the function is a dictionary. A dictionary in python has the following structure:\n",
    "\n",
    "```\n",
    "{  key1: value1,\n",
    "   key2: value2,\n",
    "   key3: value3,\n",
    "   etc.\n",
    "}\n",
    "```\n",
    "\n",
    "The dictionary created by `FreqDist()` has a word as a key and the frequency of that word as the value. Note that the words are in single quotes, because they are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(tokens)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple files in a directory\n",
    "So far, we have read one file at a time. But that's not really useful when you want to process lots of data. Here, you'll learn to use the `os` library to reall all the files in a directory, or all the files of a certain type (e.g., .txt files).\n",
    "\n",
    "The os library allows you to work with the files in your system the way you'd do in Explorer/Finder, but from inside the notebook. This [short introduction](https://www.geeksforgeeks.org/os-module-python-examples/) explains how to get the current directory, make a directory, and change directories. \n",
    "\n",
    "Here, we will only use the functions that allow us to read many files. You can see an explanation on [how to read multiple files from a folder](https://www.geeksforgeeks.org/how-to-read-multiple-text-files-from-folder-in-python/). The basics:\n",
    "\n",
    "* import the os library\n",
    "* assign the path where you want to read files from\n",
    "* use the os.path.join() function to get all the filenames\n",
    "* do a for loop to go through all the file names\n",
    "* do something with those file names (here, read them into a variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "import os\n",
    "\n",
    "# define the path\n",
    "# I'm trying to always use \"data\" as the place to store texts\n",
    "path = './data'\n",
    "\n",
    "# loop through all the files in the directory \"data\"\n",
    "for filename in os.listdir(path):\n",
    "    # check only for .txt files\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        # get all the filenames with a .txt extension\n",
    "        file_path = os.path.join(path, filename)\n",
    "        \n",
    "        # open one file at a time, to read it, and with utf encoding\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            # store the contents of the file into the variable \"text\"\n",
    "            text = f.read()\n",
    "            # the next 2 statements are just a check\n",
    "            # print the name of the file and the first 50 characters\n",
    "            print(f\"File: {filename}:\")\n",
    "            print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have read the files, let's modify that function to count the tokens and print that information. I'll modify the code above to:\n",
    "\n",
    "* create a dictionary to store the tokens\n",
    "* tokenize each text as I read it\n",
    "* store the length of the tokens in the dictionary\n",
    "\n",
    "You'll need to expand this in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "import os\n",
    "\n",
    "# define the path\n",
    "# I'm trying to always use \"data\" as the place to store texts\n",
    "path = './data'\n",
    "# create an empty dictionary. Note the curly brackets\n",
    "tokens_files = {}\n",
    "\n",
    "# loop through all the files in the directory \"data\"\n",
    "for filename in os.listdir(path):\n",
    "    # check only for .txt files\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        # get all the filenames with a .txt extension\n",
    "        file_path = os.path.join(path, filename)\n",
    "        \n",
    "        # open one file at a time, to read it, and with utf encoding\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            # store the contents of the file into the variable \"text\"\n",
    "            text = f.read()\n",
    "            # tokenize the text using NLTK\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            # store the length of the variable tokens into tokens_files\n",
    "            tokens_files[filename] = len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results. After the for loop above has completed, the dictionary `tokens_files` contains the name of the file and the length in tokens. You can print it to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's even better if you can save this in a csv file, reusing some of the code above. Then, you always have that information and don't have to run this notebook to get it. \n",
    "\n",
    "We are going to do something a little different than above for the csv information. Since we may eventually have lots of information for each file (tokens, types, lexical diversity, etc.), we'll first create a list with all the information and then we'll save it to the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remeber that you have to import the csv library, if you haven't run this from the beginning\n",
    "import csv\n",
    "\n",
    "# create an empty list\n",
    "file_info = []\n",
    "\n",
    "# save all the information from above in this list\n",
    "\n",
    "for filename in tokens_files:\n",
    "    file_info.append([\n",
    "        filename,\n",
    "        tokens_files[filename],\n",
    "    ])\n",
    "\n",
    "\n",
    "# create a csv file to store the information. I also like to put it in \"data\"\n",
    "output_csv = './data/file_info.csv'\n",
    "\n",
    "\n",
    "with open(output_csv, 'w') as out:\n",
    "    # create file writer object\n",
    "    # the delimiter is what is used to separate data into columns, in this case a comma for csv\n",
    "    writer = csv.writer(out)\n",
    "    \n",
    "    # the first row creates headings\n",
    "    writer.writerow(['file', 'tokens'])\n",
    "    \n",
    "    # write the rest\n",
    "    writer.writerows(file_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you have learned some concepts about processing text and how to deal with language data. Review the notebook and make sure you understand them:\n",
    "\n",
    "* tokens\n",
    "* types\n",
    "* lexical diversity\n",
    "* frequency distribution\n",
    "\n",
    "You have also learned a few new things about python:\n",
    "\n",
    "* 2 new data types: lists and dictionaries\n",
    "* encoding in ASCII and UTF-8\n",
    "* how to read in the contents of a file\n",
    "* how to write to a csv file\n",
    "* how to read all the files in a directory\n",
    "\n",
    "And, finally, you have also learned how to use some of NLTK's functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements\n",
    "Parts of this notebook were adapted, with thanks, from a course by [Tuomo Hiippala](https://www.mv.helsinki.fi/home/thiippal/) at the University of Helsinki."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
