{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3a5767",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb4120",
   "metadata": {},
   "source": [
    "# Normalizing data. Named entity recognition\n",
    "We have by now learned the basics of working with python and with python data types. We have also learned to process files. We are moving to doing more interesting things with textual data. In this lesson, we will learn about cleaning and normalizing data and how to identify named entities.\n",
    "\n",
    "We will continue to use NLTK, but we will also install another powerful Natural Language Processing package, spaCy. If you haven't, go to the spacy_install.ipynb notebook and follow instructions there. Then come back here to import and use spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e864925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yifangyuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yifangyuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# optional: pandas for storing information into a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# we need to import NLTK every time we want to use it\n",
    "import nltk\n",
    "\n",
    "# import the NLTK packages we know we need\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# these 2 packages may already be in your system, but just in case\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import spaCy and the small English language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# this does prettier displays on spaCy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd2db8",
   "metadata": {},
   "source": [
    "# Normalizing data\n",
    "Normalizing refers to a set of processes to make data uniform. This is generally useful to count things the correct way and to get to the essence of the words in a text. Think of counting the instances of the word \"the\" in a text. You'll want to make sure that \"the\", \"The\", and \"the?\" all look the same before you count them. Normalization includes:\n",
    "\n",
    "* Converting all words to lowercase\n",
    "* Removing or separating punctuation\n",
    "* Stemming - removing endings and ending up with the _stem_ (endings -> end; went -> went)\n",
    "* Lemmatizing - removing endings and ending up with the _root_ (endings -> end; went -> go)\n",
    "\n",
    "Most NLP packages (NLTK, spaCy) have built-in methods to do this. But it's also good to know how to do it yourself, in case you want to control what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445649a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an excerpt from The Peak, https://the-peak.ca/2025/01/sfu-study-calls-for-utility-scale-solar-power-systems-in-canada/\n",
    "text1 = \"In December 2024, Clean Energy Research Group (CERG) published a paper calling for Canada to build “mass utility-scale solar mega projects,” according to an SFU news release. Utility-scale solar “refers to large solar installations designed to feed power directly onto the electric grid.” An electric grid is an “intricate system” that provides electricity “all the way from its generation to the customers that use it for their daily needs.”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3212b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a made-up text\n",
    "text2 = \"This gotta be the wéïrdest bit of text that's ne'er gonna be The thing you'll encounter, but I have to give, gave, given, something!\\r, even the weirdest. Just tryna throw everything into a made-up bit that isn't making any sense.<br> And here's a sentence with the irregular plural feet and one with the irregular plural geese.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b6330",
   "metadata": {},
   "source": [
    "### Tokenizing without lowercase\n",
    "We will use NLTK to tokenize the texts. You can print the list of tokens, and also print the count of types and tokens. You'll see that 'And' and 'and' count as two different types. But they are really the same word. That's why we lowercase first or lowercase after tokenizing, but before counting. Compare this bit of code and the output to what happens if we lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14429b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = nltk.word_tokenize(text1)\n",
    "n_tokens1 = len(tokens1)\n",
    "n_types1 = len(set(tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c3893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'December',\n",
       " '2024',\n",
       " ',',\n",
       " 'Clean',\n",
       " 'Energy',\n",
       " 'Research',\n",
       " 'Group',\n",
       " '(',\n",
       " 'CERG',\n",
       " ')',\n",
       " 'published',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'Canada',\n",
       " 'to',\n",
       " 'build',\n",
       " '“',\n",
       " 'mass',\n",
       " 'utility-scale',\n",
       " 'solar',\n",
       " 'mega',\n",
       " 'projects',\n",
       " ',',\n",
       " '”',\n",
       " 'according',\n",
       " 'to',\n",
       " 'an',\n",
       " 'SFU',\n",
       " 'news',\n",
       " 'release',\n",
       " '.',\n",
       " 'Utility-scale',\n",
       " 'solar',\n",
       " '“',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'large',\n",
       " 'solar',\n",
       " 'installations',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'feed',\n",
       " 'power',\n",
       " 'directly',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'electric',\n",
       " 'grid.',\n",
       " '”',\n",
       " 'An',\n",
       " 'electric',\n",
       " 'grid',\n",
       " 'is',\n",
       " 'an',\n",
       " '“',\n",
       " 'intricate',\n",
       " 'system',\n",
       " '”',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'electricity',\n",
       " '“',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'from',\n",
       " 'its',\n",
       " 'generation',\n",
       " 'to',\n",
       " 'the',\n",
       " 'customers',\n",
       " 'that',\n",
       " 'use',\n",
       " 'it',\n",
       " 'for',\n",
       " 'their',\n",
       " 'daily',\n",
       " 'needs',\n",
       " '.',\n",
       " '”']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6c29ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "Canada\n",
      "all\n",
      "Energy\n",
      "intricate\n",
      "solar\n",
      "its\n",
      "generation\n",
      "customers\n",
      "Group\n",
      "CERG\n",
      "provides\n",
      "power\n",
      "from\n",
      "electric\n",
      "system\n",
      "a\n",
      "directly\n",
      "for\n",
      "2024\n",
      "refers\n",
      "“\n",
      "is\n",
      "calling\n",
      "to\n",
      "news\n",
      "onto\n",
      "according\n",
      "their\n",
      "Utility-scale\n",
      "SFU\n",
      "designed\n",
      "use\n",
      ".\n",
      "that\n",
      "utility-scale\n",
      "(\n",
      ",\n",
      "published\n",
      "”\n",
      "An\n",
      "grid\n",
      "the\n",
      "December\n",
      "mega\n",
      "build\n",
      ")\n",
      "mass\n",
      "large\n",
      "Research\n",
      "an\n",
      "daily\n",
      "way\n",
      "electricity\n",
      "paper\n",
      "release\n",
      "grid.\n",
      "feed\n",
      "needs\n",
      "projects\n",
      "Clean\n",
      "installations\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "for t in set(tokens1):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3381099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens1)\n",
    "print(n_types1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e91c596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but for text2\n",
    "tokens2 = nltk.word_tokenize(text2)\n",
    "n_tokens2 = len(tokens2)\n",
    "n_types2 = len(set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7381173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'be',\n",
       " 'the',\n",
       " 'wéïrdest',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'text',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"ne'er\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'be',\n",
       " 'The',\n",
       " 'thing',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'encounter',\n",
       " ',',\n",
       " 'but',\n",
       " 'I',\n",
       " 'have',\n",
       " 'to',\n",
       " 'give',\n",
       " ',',\n",
       " 'gave',\n",
       " ',',\n",
       " 'given',\n",
       " ',',\n",
       " 'something',\n",
       " '!',\n",
       " ',',\n",
       " 'even',\n",
       " 'the',\n",
       " 'weirdest',\n",
       " '.',\n",
       " 'Just',\n",
       " 'tryna',\n",
       " 'throw',\n",
       " 'everything',\n",
       " 'into',\n",
       " 'a',\n",
       " 'made-up',\n",
       " 'bit',\n",
       " 'that',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'making',\n",
       " 'any',\n",
       " 'sense.',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'And',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'feet',\n",
       " 'and',\n",
       " 'one',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'geese',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdfdf730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "into\n",
      "thing\n",
      "weirdest\n",
      "made-up\n",
      "irregular\n",
      "plural\n",
      "any\n",
      "I\n",
      "!\n",
      "feet\n",
      "something\n",
      "you\n",
      "tryna\n",
      "'s\n",
      "<\n",
      "here\n",
      "geese\n",
      "And\n",
      "a\n",
      "ta\n",
      "got\n",
      "is\n",
      "'ll\n",
      "ne'er\n",
      "to\n",
      "even\n",
      "na\n",
      "that\n",
      ".\n",
      ">\n",
      ",\n",
      "throw\n",
      "have\n",
      "given\n",
      "and\n",
      "bit\n",
      "making\n",
      "sense.\n",
      "encounter\n",
      "the\n",
      "give\n",
      "one\n",
      "Just\n",
      "This\n",
      "text\n",
      "gon\n",
      "gave\n",
      "of\n",
      "sentence\n",
      "be\n",
      "with\n",
      "everything\n",
      "but\n",
      "n't\n",
      "br\n",
      "wéïrdest\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "for t in set(tokens2):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9fda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens2)\n",
    "print(n_types2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43723a47",
   "metadata": {},
   "source": [
    "### Tokenizing after lowercasing\n",
    "Compare the numbers and the output now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac82197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using the lower() method in a string\n",
    "tokens1_lower = [w.lower() for w in tokens1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af02e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'december',\n",
       " '2024',\n",
       " ',',\n",
       " 'clean',\n",
       " 'energy',\n",
       " 'research',\n",
       " 'group',\n",
       " '(',\n",
       " 'cerg',\n",
       " ')',\n",
       " 'published',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'canada',\n",
       " 'to',\n",
       " 'build',\n",
       " '“',\n",
       " 'mass',\n",
       " 'utility-scale',\n",
       " 'solar',\n",
       " 'mega',\n",
       " 'projects',\n",
       " ',',\n",
       " '”',\n",
       " 'according',\n",
       " 'to',\n",
       " 'an',\n",
       " 'sfu',\n",
       " 'news',\n",
       " 'release',\n",
       " '.',\n",
       " 'utility-scale',\n",
       " 'solar',\n",
       " '“',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'large',\n",
       " 'solar',\n",
       " 'installations',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'feed',\n",
       " 'power',\n",
       " 'directly',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'electric',\n",
       " 'grid.',\n",
       " '”',\n",
       " 'an',\n",
       " 'electric',\n",
       " 'grid',\n",
       " 'is',\n",
       " 'an',\n",
       " '“',\n",
       " 'intricate',\n",
       " 'system',\n",
       " '”',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'electricity',\n",
       " '“',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'from',\n",
       " 'its',\n",
       " 'generation',\n",
       " 'to',\n",
       " 'the',\n",
       " 'customers',\n",
       " 'that',\n",
       " 'use',\n",
       " 'it',\n",
       " 'for',\n",
       " 'their',\n",
       " 'daily',\n",
       " 'needs',\n",
       " '.',\n",
       " '”']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa03f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_types1_lower = len(set(tokens1_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f00d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "63\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens1)\n",
    "print(n_types1)\n",
    "print(n_types1_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c993812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for text 2\n",
    "tokens2_lower = [w.lower() for w in tokens2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf8f8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'be',\n",
       " 'the',\n",
       " 'wéïrdest',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'text',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"ne'er\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'be',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'encounter',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'give',\n",
       " ',',\n",
       " 'gave',\n",
       " ',',\n",
       " 'given',\n",
       " ',',\n",
       " 'something',\n",
       " '!',\n",
       " ',',\n",
       " 'even',\n",
       " 'the',\n",
       " 'weirdest',\n",
       " '.',\n",
       " 'just',\n",
       " 'tryna',\n",
       " 'throw',\n",
       " 'everything',\n",
       " 'into',\n",
       " 'a',\n",
       " 'made-up',\n",
       " 'bit',\n",
       " 'that',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'making',\n",
       " 'any',\n",
       " 'sense.',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'and',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'feet',\n",
       " 'and',\n",
       " 'one',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'geese',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9fbd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_types2_lower = len(set(tokens2_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd61c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "57\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens2)\n",
    "print(n_types2)\n",
    "print(n_types2_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15fe529",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing with NLTK\n",
    "\n",
    "### Stemming\n",
    "Stemmers remove any endings that may be [inflectional suffixes](https://en.wikipedia.org/wiki/Inflection) in English. There are different versions of stemmers, even within NLTK. See the [overview of stemming in NLTK](https://www.nltk.org/howto/stem.html). Here, we'll use the [Porter Stemmer](https://www.nltk.org/_modules/nltk/stem/porter.html), developed by Martin Porter. \n",
    "\n",
    "Look at the output carefully and note where things don't seem to make sense. This is because Porter removes anything that may possibly be an ending, including the '-er' in 'December', because it is sometimes an inflectional ending in words like 'clever'.\n",
    "\n",
    "### Lemmatization\n",
    "Lemmatizers are a bit smarter about inflection and are able to identify roots, even when no suffixes are involved (gave -> give; feet -> foot). We'll use the [WordNet lemmatizer](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet) in NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5dc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the stemmer to a variable, 'stemmer'\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# go through the list of tokens (tokens1)\n",
    "# lower the tokens in that list\n",
    "# use list comprehension (with the square brackets)\n",
    " # so that the stemmer can iterate over the list\n",
    "tokens1_st = [stemmer.stem(token.lower()) for token in tokens1]\n",
    "tokens2_st = [stemmer.stem(token.lower()) for token in tokens2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a155dfe7",
   "metadata": {},
   "source": [
    "tokens1_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece79e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'be',\n",
       " 'the',\n",
       " 'wéïrdest',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'text',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"ne'er\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'be',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'encount',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'give',\n",
       " ',',\n",
       " 'gave',\n",
       " ',',\n",
       " 'given',\n",
       " ',',\n",
       " 'someth',\n",
       " '!',\n",
       " ',',\n",
       " 'even',\n",
       " 'the',\n",
       " 'weirdest',\n",
       " '.',\n",
       " 'just',\n",
       " 'tryna',\n",
       " 'throw',\n",
       " 'everyth',\n",
       " 'into',\n",
       " 'a',\n",
       " 'made-up',\n",
       " 'bit',\n",
       " 'that',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'make',\n",
       " 'ani',\n",
       " 'sense.',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'and',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'sentenc',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'feet',\n",
       " 'and',\n",
       " 'one',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'gees',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df2e9eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'December',\n",
       " '2024',\n",
       " ',',\n",
       " 'Clean',\n",
       " 'Energy',\n",
       " 'Research',\n",
       " 'Group',\n",
       " '(',\n",
       " 'CERG',\n",
       " ')',\n",
       " 'published',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'Canada',\n",
       " 'to',\n",
       " 'build',\n",
       " '“',\n",
       " 'mass',\n",
       " 'utility-scale',\n",
       " 'solar',\n",
       " 'mega',\n",
       " 'projects',\n",
       " ',',\n",
       " '”',\n",
       " 'according',\n",
       " 'to',\n",
       " 'an',\n",
       " 'SFU',\n",
       " 'news',\n",
       " 'release',\n",
       " '.',\n",
       " 'Utility-scale',\n",
       " 'solar',\n",
       " '“',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'large',\n",
       " 'solar',\n",
       " 'installations',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'feed',\n",
       " 'power',\n",
       " 'directly',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'electric',\n",
       " 'grid.',\n",
       " '”',\n",
       " 'An',\n",
       " 'electric',\n",
       " 'grid',\n",
       " 'is',\n",
       " 'an',\n",
       " '“',\n",
       " 'intricate',\n",
       " 'system',\n",
       " '”',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'electricity',\n",
       " '“',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'from',\n",
       " 'its',\n",
       " 'generation',\n",
       " 'to',\n",
       " 'the',\n",
       " 'customers',\n",
       " 'that',\n",
       " 'use',\n",
       " 'it',\n",
       " 'for',\n",
       " 'their',\n",
       " 'daily',\n",
       " 'needs',\n",
       " '.',\n",
       " '”']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70a1eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the lemmatizer to a variable\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# go through the list of tokens and lemmatize\n",
    "tokens1_lm = [lemmatizer.lemmatize(token.lower()) for token in tokens1]\n",
    "tokens2_lm = [lemmatizer.lemmatize(token.lower()) for token in tokens2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06723ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'december',\n",
       " '2024',\n",
       " ',',\n",
       " 'clean',\n",
       " 'energy',\n",
       " 'research',\n",
       " 'group',\n",
       " '(',\n",
       " 'cerg',\n",
       " ')',\n",
       " 'published',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'canada',\n",
       " 'to',\n",
       " 'build',\n",
       " '“',\n",
       " 'mass',\n",
       " 'utility-scale',\n",
       " 'solar',\n",
       " 'mega',\n",
       " 'project',\n",
       " ',',\n",
       " '”',\n",
       " 'according',\n",
       " 'to',\n",
       " 'an',\n",
       " 'sfu',\n",
       " 'news',\n",
       " 'release',\n",
       " '.',\n",
       " 'utility-scale',\n",
       " 'solar',\n",
       " '“',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'large',\n",
       " 'solar',\n",
       " 'installation',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'feed',\n",
       " 'power',\n",
       " 'directly',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'electric',\n",
       " 'grid.',\n",
       " '”',\n",
       " 'an',\n",
       " 'electric',\n",
       " 'grid',\n",
       " 'is',\n",
       " 'an',\n",
       " '“',\n",
       " 'intricate',\n",
       " 'system',\n",
       " '”',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'electricity',\n",
       " '“',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'from',\n",
       " 'it',\n",
       " 'generation',\n",
       " 'to',\n",
       " 'the',\n",
       " 'customer',\n",
       " 'that',\n",
       " 'use',\n",
       " 'it',\n",
       " 'for',\n",
       " 'their',\n",
       " 'daily',\n",
       " 'need',\n",
       " '.',\n",
       " '”']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2db0434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'be',\n",
       " 'the',\n",
       " 'wéïrdest',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'text',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"ne'er\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'be',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'encounter',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'give',\n",
       " ',',\n",
       " 'gave',\n",
       " ',',\n",
       " 'given',\n",
       " ',',\n",
       " 'something',\n",
       " '!',\n",
       " ',',\n",
       " 'even',\n",
       " 'the',\n",
       " 'weirdest',\n",
       " '.',\n",
       " 'just',\n",
       " 'tryna',\n",
       " 'throw',\n",
       " 'everything',\n",
       " 'into',\n",
       " 'a',\n",
       " 'made-up',\n",
       " 'bit',\n",
       " 'that',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'making',\n",
       " 'any',\n",
       " 'sense.',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'and',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'foot',\n",
       " 'and',\n",
       " 'one',\n",
       " 'with',\n",
       " 'the',\n",
       " 'irregular',\n",
       " 'plural',\n",
       " 'goose',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b37813",
   "metadata": {},
   "source": [
    "### Other options: remove non-ASCII characters, remove HTML tags\n",
    "There are additional things you may want to do to clean and normalize text, including converting everything to ASCII (wéïrdest -> weirdest) and removing HTML tags ('`<br>`', -> '' ). You'll probably want to do that before tokenization, so that the angle brackets in HTML don't get tokenized as punctuation. \n",
    "\n",
    "The [emoji](https://pypi.org/project/emoji/) library can also convert UTF emoji into descriptions (&#x1F917; -> 'hugging face')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cff14",
   "metadata": {},
   "source": [
    "## Normalizing text with spaCy\n",
    "Now that you have seen how to do this with NLTK, the good news is that spaCy will do pretty much everything you need to do to clean and normalize text. It will also give you morphological and syntactic information about it. Go to the `spacy_install.ipynb` notebook for more information on how spaCy works.\n",
    "\n",
    "We call spaCy by using the `nlp` object and passing it the text that we want to process. Then we can query and print the information contained in the `doc` object that spaCy creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b24d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the text with spaCy\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bed9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "December\n",
      "2024\n",
      ",\n",
      "Clean\n",
      "Energy\n",
      "Research\n",
      "Group\n",
      "(\n",
      "CERG\n",
      ")\n",
      "published\n",
      "a\n",
      "paper\n",
      "calling\n",
      "for\n",
      "Canada\n",
      "to\n",
      "build\n",
      "“\n",
      "mass\n",
      "utility\n",
      "-\n",
      "scale\n",
      "solar\n",
      "mega\n",
      "projects\n",
      ",\n",
      "”\n",
      "according\n",
      "to\n",
      "an\n",
      "SFU\n",
      "news\n",
      "release\n",
      ".\n",
      "Utility\n",
      "-\n",
      "scale\n",
      "solar\n",
      "“\n",
      "refers\n",
      "to\n",
      "large\n",
      "solar\n",
      "installations\n",
      "designed\n",
      "to\n",
      "feed\n",
      "power\n",
      "directly\n",
      "onto\n",
      "the\n",
      "electric\n",
      "grid\n",
      ".\n",
      "”\n",
      "An\n",
      "electric\n",
      "grid\n",
      "is\n",
      "an\n",
      "“\n",
      "intricate\n",
      "system\n",
      "”\n",
      "that\n",
      "provides\n",
      "electricity\n",
      "“\n",
      "all\n",
      "the\n",
      "way\n",
      "from\n",
      "its\n",
      "generation\n",
      "to\n",
      "the\n",
      "customers\n",
      "that\n",
      "use\n",
      "it\n",
      "for\n",
      "their\n",
      "daily\n",
      "needs\n",
      ".\n",
      "”\n"
     ]
    }
   ],
   "source": [
    "# print the 'text' attribute of each of the tokens\n",
    "for token in doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f0bb5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In December 2024 , Clean Energy Research Group ( CERG ) published a paper calling for Canada to build “ mass utility - scale solar mega projects , ” according to an SFU news release . Utility - scale solar “ refers to large solar installations designed to feed power directly onto the electric grid . ” An electric grid is an “ intricate system ” that provides electricity “ all the way from its generation to the customers that use it for their daily needs . ”\n",
      "\r\n",
      "This got ta be the wéïrdest bit of text that 's ne'er gon na be The thing you 'll encounter , but I have to give , gave , given , something ! \r",
      " , even the weirdest . Just tryna throw everything into a made - up bit that is n't making any sense.<br > And here 's a sentence with the irregular plural feet and one with the irregular plural geese .\n"
     ]
    }
   ],
   "source": [
    "# if you want to see this in a single line, you can join the strings in the list, with a space between each\n",
    "print(\" \".join([token.text for token in doc1]))\n",
    "print(\"\\r\") # this is just so that the two texts are separated on the screen by an empty line\n",
    "print(\" \".join([token.text for token in doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c35b1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in December 2024 , Clean Energy Research Group ( CERG ) publish a paper call for Canada to build \" mass utility - scale solar mega project , \" accord to an SFU news release . utility - scale solar \" refer to large solar installation design to feed power directly onto the electric grid . \" an electric grid be an \" intricate system \" that provide electricity \" all the way from its generation to the customer that use it for their daily need . \"\n",
      "\r\n",
      "this get to be the wéïrd bit of text that be ne'er go to be the thing you will encounter , but I have to give , give , give , something ! \r",
      " , even the weird . just tryna throw everything into a make - up bit that be not make any sense.<br > and here be a sentence with the irregular plural foot and one with the irregular plural geese .\n"
     ]
    }
   ],
   "source": [
    "# print the lemmas\n",
    "print(\" \".join([token.lemma_ for token in doc1]))\n",
    "print(\"\\r\")\n",
    "print(\" \".join([token.lemma_ for token in doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dad0beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In/ADP December/PROPN 2024/NUM ,/PUNCT Clean/PROPN Energy/PROPN Research/PROPN Group/PROPN (/PUNCT CERG/PROPN )/PUNCT published/VERB a/DET paper/NOUN calling/VERB for/SCONJ Canada/PROPN to/PART build/VERB “/PUNCT mass/ADJ utility/NOUN -/PUNCT scale/NOUN solar/ADJ mega/ADJ projects/NOUN ,/PUNCT ”/PUNCT according/VERB to/ADP an/DET SFU/PROPN news/NOUN release/NOUN ./PUNCT Utility/NOUN -/PUNCT scale/NOUN solar/NOUN “/PUNCT refers/VERB to/ADP large/ADJ solar/ADJ installations/NOUN designed/VERB to/PART feed/VERB power/NOUN directly/ADV onto/ADP the/DET electric/ADJ grid/NOUN ./PUNCT ”/PUNCT An/DET electric/ADJ grid/NOUN is/AUX an/DET “/PUNCT intricate/ADJ system/NOUN ”/PUNCT that/PRON provides/VERB electricity/NOUN “/PUNCT all/DET the/DET way/NOUN from/ADP its/PRON generation/NOUN to/ADP the/DET customers/NOUN that/PRON use/VERB it/PRON for/ADP their/PRON daily/ADJ needs/NOUN ./PUNCT ”/PUNCT\n",
      "\r\n",
      "This/PRON got/VERB ta/PART be/AUX the/DET wéïrdest/ADJ bit/NOUN of/ADP text/NOUN that/PRON 's/AUX ne'er/NOUN gon/VERB na/PART be/AUX The/DET thing/NOUN you/PRON 'll/AUX encounter/VERB ,/PUNCT but/CCONJ I/PRON have/VERB to/PART give/VERB ,/PUNCT gave/VERB ,/PUNCT given/VERB ,/PUNCT something/PRON !/PUNCT \r",
      "/SPACE ,/PUNCT even/ADV the/DET weirdest/ADJ ./PUNCT Just/ADV tryna/NOUN throw/VERB everything/PRON into/ADP a/DET made/VERB -/PUNCT up/ADP bit/NOUN that/PRON is/AUX n't/PART making/VERB any/DET sense.<br/X >/X And/CCONJ here/ADV 's/AUX a/DET sentence/NOUN with/ADP the/DET irregular/ADJ plural/ADJ feet/NOUN and/CCONJ one/NUM with/ADP the/DET irregular/ADJ plural/ADJ geese/PROPN ./PUNCT\n"
     ]
    }
   ],
   "source": [
    "# print the part of speech of each word after the word\n",
    "print(\" \".join([f\"{token.text}/{token.pos_}\" for token in doc1]))\n",
    "print(\"\\r\")\n",
    "print(\" \".join([f\"{token.text}/{token.pos_}\" for token in doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f69ece49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In \t\t in \t\t ADP \t\t \n",
      "December \t\t December \t\t PROPN \t\t Number=Sing\n",
      "2024 \t\t 2024 \t\t NUM \t\t NumType=Card\n",
      ", \t\t , \t\t PUNCT \t\t PunctType=Comm\n",
      "Clean \t\t Clean \t\t PROPN \t\t Number=Sing\n",
      "Energy \t\t Energy \t\t PROPN \t\t Number=Sing\n",
      "Research \t\t Research \t\t PROPN \t\t Number=Sing\n",
      "Group \t\t Group \t\t PROPN \t\t Number=Sing\n",
      "( \t\t ( \t\t PUNCT \t\t PunctSide=Ini|PunctType=Brck\n",
      "CERG \t\t CERG \t\t PROPN \t\t Number=Sing\n",
      ") \t\t ) \t\t PUNCT \t\t PunctSide=Fin|PunctType=Brck\n",
      "published \t\t publish \t\t VERB \t\t Tense=Past|VerbForm=Fin\n",
      "a \t\t a \t\t DET \t\t Definite=Ind|PronType=Art\n",
      "paper \t\t paper \t\t NOUN \t\t Number=Sing\n",
      "calling \t\t call \t\t VERB \t\t Aspect=Prog|Tense=Pres|VerbForm=Part\n",
      "for \t\t for \t\t SCONJ \t\t \n",
      "Canada \t\t Canada \t\t PROPN \t\t Number=Sing\n",
      "to \t\t to \t\t PART \t\t \n",
      "build \t\t build \t\t VERB \t\t VerbForm=Inf\n",
      "“ \t\t \" \t\t PUNCT \t\t PunctSide=Ini|PunctType=Quot\n",
      "mass \t\t mass \t\t ADJ \t\t Degree=Pos\n",
      "utility \t\t utility \t\t NOUN \t\t Number=Sing\n",
      "- \t\t - \t\t PUNCT \t\t PunctType=Dash\n",
      "scale \t\t scale \t\t NOUN \t\t Number=Sing\n",
      "solar \t\t solar \t\t ADJ \t\t Degree=Pos\n",
      "mega \t\t mega \t\t ADJ \t\t Degree=Pos\n",
      "projects \t\t project \t\t NOUN \t\t Number=Plur\n",
      ", \t\t , \t\t PUNCT \t\t PunctType=Comm\n",
      "” \t\t \" \t\t PUNCT \t\t PunctSide=Fin|PunctType=Quot\n",
      "according \t\t accord \t\t VERB \t\t Aspect=Prog|Tense=Pres|VerbForm=Part\n",
      "to \t\t to \t\t ADP \t\t \n",
      "an \t\t an \t\t DET \t\t Definite=Ind|PronType=Art\n",
      "SFU \t\t SFU \t\t PROPN \t\t Number=Sing\n",
      "news \t\t news \t\t NOUN \t\t Number=Sing\n",
      "release \t\t release \t\t NOUN \t\t Number=Sing\n",
      ". \t\t . \t\t PUNCT \t\t PunctType=Peri\n",
      "Utility \t\t utility \t\t NOUN \t\t Number=Sing\n",
      "- \t\t - \t\t PUNCT \t\t PunctType=Dash\n",
      "scale \t\t scale \t\t NOUN \t\t Number=Sing\n",
      "solar \t\t solar \t\t NOUN \t\t Number=Sing\n",
      "“ \t\t \" \t\t PUNCT \t\t PunctSide=Ini|PunctType=Quot\n",
      "refers \t\t refer \t\t VERB \t\t Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "to \t\t to \t\t ADP \t\t \n",
      "large \t\t large \t\t ADJ \t\t Degree=Pos\n",
      "solar \t\t solar \t\t ADJ \t\t Degree=Pos\n",
      "installations \t\t installation \t\t NOUN \t\t Number=Plur\n",
      "designed \t\t design \t\t VERB \t\t Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "to \t\t to \t\t PART \t\t \n",
      "feed \t\t feed \t\t VERB \t\t VerbForm=Inf\n",
      "power \t\t power \t\t NOUN \t\t Number=Sing\n",
      "directly \t\t directly \t\t ADV \t\t \n",
      "onto \t\t onto \t\t ADP \t\t \n",
      "the \t\t the \t\t DET \t\t Definite=Def|PronType=Art\n",
      "electric \t\t electric \t\t ADJ \t\t Degree=Pos\n",
      "grid \t\t grid \t\t NOUN \t\t Number=Sing\n",
      ". \t\t . \t\t PUNCT \t\t PunctType=Peri\n",
      "” \t\t \" \t\t PUNCT \t\t PunctSide=Fin|PunctType=Quot\n",
      "An \t\t an \t\t DET \t\t Definite=Ind|PronType=Art\n",
      "electric \t\t electric \t\t ADJ \t\t Degree=Pos\n",
      "grid \t\t grid \t\t NOUN \t\t Number=Sing\n",
      "is \t\t be \t\t AUX \t\t Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "an \t\t an \t\t DET \t\t Definite=Ind|PronType=Art\n",
      "“ \t\t \" \t\t PUNCT \t\t PunctSide=Ini|PunctType=Quot\n",
      "intricate \t\t intricate \t\t ADJ \t\t Degree=Pos\n",
      "system \t\t system \t\t NOUN \t\t Number=Sing\n",
      "” \t\t \" \t\t PUNCT \t\t PunctSide=Fin|PunctType=Quot\n",
      "that \t\t that \t\t PRON \t\t PronType=Rel\n",
      "provides \t\t provide \t\t VERB \t\t Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "electricity \t\t electricity \t\t NOUN \t\t Number=Sing\n",
      "“ \t\t \" \t\t PUNCT \t\t PunctSide=Ini|PunctType=Quot\n",
      "all \t\t all \t\t DET \t\t \n",
      "the \t\t the \t\t DET \t\t Definite=Def|PronType=Art\n",
      "way \t\t way \t\t NOUN \t\t Number=Sing\n",
      "from \t\t from \t\t ADP \t\t \n",
      "its \t\t its \t\t PRON \t\t Gender=Neut|Number=Sing|Person=3|Poss=Yes|PronType=Prs\n",
      "generation \t\t generation \t\t NOUN \t\t Number=Sing\n",
      "to \t\t to \t\t ADP \t\t \n",
      "the \t\t the \t\t DET \t\t Definite=Def|PronType=Art\n",
      "customers \t\t customer \t\t NOUN \t\t Number=Plur\n",
      "that \t\t that \t\t PRON \t\t PronType=Rel\n",
      "use \t\t use \t\t VERB \t\t Tense=Pres|VerbForm=Fin\n",
      "it \t\t it \t\t PRON \t\t Case=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "for \t\t for \t\t ADP \t\t \n",
      "their \t\t their \t\t PRON \t\t Number=Plur|Person=3|Poss=Yes|PronType=Prs\n",
      "daily \t\t daily \t\t ADJ \t\t Degree=Pos\n",
      "needs \t\t need \t\t NOUN \t\t Number=Plur\n",
      ". \t\t . \t\t PUNCT \t\t PunctType=Peri\n",
      "” \t\t \" \t\t PUNCT \t\t PunctSide=Fin|PunctType=Quot\n"
     ]
    }
   ],
   "source": [
    "# the doc object in spaCy contains all kinds of information\n",
    "# including rich morphology for each word\n",
    "for token in doc1:\n",
    "    print(token.text, \"\\t\\t\", token.lemma_, \"\\t\\t\", token.pos_, \"\\t\\t\", token.morph) # the \\t helps show sort of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28406f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you don't know what the abbreviations mean, \n",
    "# you can ask for an explanation\n",
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d7831d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In \t\t ADP \t\t prep \t\t published\n",
      "December \t\t PROPN \t\t pobj \t\t In\n",
      "2024 \t\t NUM \t\t nummod \t\t December\n",
      ", \t\t PUNCT \t\t punct \t\t published\n",
      "Clean \t\t PROPN \t\t compound \t\t Energy\n",
      "Energy \t\t PROPN \t\t compound \t\t Group\n",
      "Research \t\t PROPN \t\t compound \t\t Group\n",
      "Group \t\t PROPN \t\t nsubj \t\t published\n",
      "( \t\t PUNCT \t\t punct \t\t Group\n",
      "CERG \t\t PROPN \t\t appos \t\t Group\n",
      ") \t\t PUNCT \t\t punct \t\t Group\n",
      "published \t\t VERB \t\t ROOT \t\t published\n",
      "a \t\t DET \t\t det \t\t paper\n",
      "paper \t\t NOUN \t\t dobj \t\t published\n",
      "calling \t\t VERB \t\t acl \t\t paper\n",
      "for \t\t SCONJ \t\t mark \t\t build\n",
      "Canada \t\t PROPN \t\t nsubj \t\t build\n",
      "to \t\t PART \t\t aux \t\t build\n",
      "build \t\t VERB \t\t advcl \t\t calling\n",
      "“ \t\t PUNCT \t\t punct \t\t projects\n",
      "mass \t\t ADJ \t\t amod \t\t projects\n",
      "utility \t\t NOUN \t\t nmod \t\t scale\n",
      "- \t\t PUNCT \t\t punct \t\t scale\n",
      "scale \t\t NOUN \t\t nmod \t\t projects\n",
      "solar \t\t ADJ \t\t amod \t\t mega\n",
      "mega \t\t ADJ \t\t compound \t\t projects\n",
      "projects \t\t NOUN \t\t dobj \t\t build\n",
      ", \t\t PUNCT \t\t punct \t\t published\n",
      "” \t\t PUNCT \t\t punct \t\t published\n",
      "according \t\t VERB \t\t prep \t\t published\n",
      "to \t\t ADP \t\t prep \t\t according\n",
      "an \t\t DET \t\t det \t\t release\n",
      "SFU \t\t PROPN \t\t compound \t\t release\n",
      "news \t\t NOUN \t\t compound \t\t release\n",
      "release \t\t NOUN \t\t pobj \t\t to\n",
      ". \t\t PUNCT \t\t punct \t\t published\n",
      "Utility \t\t NOUN \t\t compound \t\t scale\n",
      "- \t\t PUNCT \t\t punct \t\t scale\n",
      "scale \t\t NOUN \t\t compound \t\t solar\n",
      "solar \t\t NOUN \t\t nsubj \t\t refers\n",
      "“ \t\t PUNCT \t\t punct \t\t refers\n",
      "refers \t\t VERB \t\t ROOT \t\t refers\n",
      "to \t\t ADP \t\t prep \t\t refers\n",
      "large \t\t ADJ \t\t amod \t\t installations\n",
      "solar \t\t ADJ \t\t amod \t\t installations\n",
      "installations \t\t NOUN \t\t pobj \t\t to\n",
      "designed \t\t VERB \t\t acl \t\t installations\n",
      "to \t\t PART \t\t aux \t\t feed\n",
      "feed \t\t VERB \t\t xcomp \t\t designed\n",
      "power \t\t NOUN \t\t dobj \t\t feed\n",
      "directly \t\t ADV \t\t advmod \t\t onto\n",
      "onto \t\t ADP \t\t prep \t\t feed\n",
      "the \t\t DET \t\t det \t\t grid\n",
      "electric \t\t ADJ \t\t amod \t\t grid\n",
      "grid \t\t NOUN \t\t pobj \t\t onto\n",
      ". \t\t PUNCT \t\t punct \t\t refers\n",
      "” \t\t PUNCT \t\t punct \t\t refers\n",
      "An \t\t DET \t\t det \t\t grid\n",
      "electric \t\t ADJ \t\t amod \t\t grid\n",
      "grid \t\t NOUN \t\t nsubj \t\t is\n",
      "is \t\t AUX \t\t ROOT \t\t is\n",
      "an \t\t DET \t\t det \t\t system\n",
      "“ \t\t PUNCT \t\t punct \t\t system\n",
      "intricate \t\t ADJ \t\t amod \t\t system\n",
      "system \t\t NOUN \t\t attr \t\t is\n",
      "” \t\t PUNCT \t\t punct \t\t system\n",
      "that \t\t PRON \t\t nsubj \t\t provides\n",
      "provides \t\t VERB \t\t relcl \t\t system\n",
      "electricity \t\t NOUN \t\t dobj \t\t provides\n",
      "“ \t\t PUNCT \t\t punct \t\t system\n",
      "all \t\t DET \t\t predet \t\t way\n",
      "the \t\t DET \t\t det \t\t way\n",
      "way \t\t NOUN \t\t npadvmod \t\t from\n",
      "from \t\t ADP \t\t prep \t\t system\n",
      "its \t\t PRON \t\t poss \t\t generation\n",
      "generation \t\t NOUN \t\t pobj \t\t from\n",
      "to \t\t ADP \t\t prep \t\t from\n",
      "the \t\t DET \t\t det \t\t customers\n",
      "customers \t\t NOUN \t\t pobj \t\t to\n",
      "that \t\t PRON \t\t nsubj \t\t use\n",
      "use \t\t VERB \t\t relcl \t\t customers\n",
      "it \t\t PRON \t\t dobj \t\t use\n",
      "for \t\t ADP \t\t prep \t\t use\n",
      "their \t\t PRON \t\t poss \t\t needs\n",
      "daily \t\t ADJ \t\t amod \t\t needs\n",
      "needs \t\t NOUN \t\t pobj \t\t for\n",
      ". \t\t PUNCT \t\t punct \t\t is\n",
      "” \t\t PUNCT \t\t punct \t\t is\n"
     ]
    }
   ],
   "source": [
    "# doc also includes syntactic information about heads and dependents\n",
    "# including rich morphology for each word\n",
    "for token in doc1:\n",
    "    print(token.text, \"\\t\\t\", token.pos_, \"\\t\\t\", token.dep_, \"\\t\\t\", token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e17d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  ['In', 'December', '2024', ',', 'Clean', 'Energy', 'Research', 'Group', '(', 'CERG', ')', 'published', 'a', 'paper', 'calling', 'for', 'Canada', 'to', 'build', '“', 'mass', 'utility', '-', 'scale', 'solar', 'mega', 'projects', ',', '”', 'according', 'to', 'an', 'SFU', 'news', 'release', '.', 'Utility', '-', 'scale', 'solar', '“', 'refers', 'to', 'large', 'solar', 'installations', 'designed', 'to', 'feed', 'power', 'directly', 'onto', 'the', 'electric', 'grid', '.', '”', 'An', 'electric', 'grid', 'is', 'an', '“', 'intricate', 'system', '”', 'that', 'provides', 'electricity', '“', 'all', 'the', 'way', 'from', 'its', 'generation', 'to', 'the', 'customers', 'that', 'use', 'it', 'for', 'their', 'daily', 'needs', '.', '”']\n",
      "\r\n",
      "types:  {'In', 'Canada', '-', 'Energy', 'intricate', 'all', 'solar', 'its', 'generation', 'customers', 'scale', 'Group', 'CERG', 'provides', 'power', 'from', 'electric', 'system', 'a', 'directly', 'for', '2024', 'refers', '“', 'is', 'calling', 'to', 'news', 'onto', 'according', 'their', 'SFU', 'designed', 'use', '.', 'that', '(', ',', 'published', '”', 'An', 'grid', 'Utility', 'the', 'December', 'utility', 'mega', 'build', ')', 'mass', 'large', 'Research', 'an', 'daily', 'way', 'electricity', 'paper', 'release', 'needs', 'feed', 'projects', 'Clean', 'installations', 'it'}\n",
      "\r\n",
      "number of tokens:  88\n",
      "number of types:  64\n"
     ]
    }
   ],
   "source": [
    "# btw, you can still count tokens and types with spaCy\n",
    "tokens1 = [token.text for token in doc1]\n",
    "types1 = set(tokens1)\n",
    "\n",
    "# you can see those lists\n",
    "print(\"tokens: \", tokens1)\n",
    "print(\"\\r\")\n",
    "print(\"types: \", types1)\n",
    "\n",
    "# and you can print their length\n",
    "print(\"\\r\")\n",
    "print(\"number of tokens: \", len(tokens1))\n",
    "print(\"number of types: \", len(types1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06687f60",
   "metadata": {},
   "source": [
    "### Using pandas to show the output\n",
    "You can also store the information into a pandas dataframe, which makes it much more readable and easy to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e86c2d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>December</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nummod</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clean</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>their</td>\n",
       "      <td>PRON</td>\n",
       "      <td>poss</td>\n",
       "      <td>needs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>daily</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>needs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>needs</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>”</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text    Tag Dependency       Head\n",
       "0         In    ADP       prep  published\n",
       "1   December  PROPN       pobj         In\n",
       "2       2024    NUM     nummod   December\n",
       "3          ,  PUNCT      punct  published\n",
       "4      Clean  PROPN   compound     Energy\n",
       "..       ...    ...        ...        ...\n",
       "83     their   PRON       poss      needs\n",
       "84     daily    ADJ       amod      needs\n",
       "85     needs   NOUN       pobj        for\n",
       "86         .  PUNCT      punct         is\n",
       "87         ”  PUNCT      punct         is\n",
       "\n",
       "[88 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = []\n",
    "\n",
    "for token in doc1:\n",
    "    data1.append([token.text, token.pos_, token.dep_, token.head])\n",
    "    \n",
    "df = pd.DataFrame(data1)\n",
    "df.columns = ['Text', 'Tag', 'Dependency', 'Head']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5fe09e",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "Named Entity Recognition (NER) is the process of identifying and labelling _named entities_, that is, real world objects, locations, and identifiers such as dates, currency, or quantities. It is very useful if you want to know, for instance, who is mentioned in a text, which countries are involved, or which dates. It is what allows your email or messaging application to identify dates and suggest a calendar entry, as in the image below, from the iPhone Notes app. \n",
    "\n",
    "\n",
    "<a href=\"./img/ner_date.jpeg\" target=\"_blank\">\n",
    "<img src=\"./img/ner_date.jpeg\" width=\"100\" height=\"200\" style=\"border: 1px solid gray; padding: 5px;\"> </a>\n",
    "\n",
    "spaCy has a pretty powerful NER module. It can give you the named entities in a text, with the label for each word that is part of the entity. It also knows the boundaries of the entire entity. So it knows that \"Clean\", \"Energy\", \"Research\", and \"Group\" all have the ORG (for organization) label, but it also knows that the full entity is \"Clean Energy Research Group\". \n",
    "\n",
    "You can list all the entities in a text with the usual for loop. Using the [displacy module](https://spacy.io/usage/visualizers), you can also visualize the boundaries and the types in different colours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e9fd9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 2024 DATE\n",
      "Clean Energy Research Group ORG\n",
      "CERG ORG\n",
      "Canada GPE\n",
      "SFU ORG\n",
      "daily DATE\n"
     ]
    }
   ],
   "source": [
    "# print each token and its entity label, if it has one\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "040c9973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geese NORP\n"
     ]
    }
   ],
   "source": [
    "# same for doc2, but it doesn't contain entities\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2706ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('December 2024', 'DATE'), ('Clean Energy Research Group', 'ORG'), ('CERG', 'ORG'), ('Canada', 'GPE'), ('SFU', 'ORG'), ('daily', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "# it's useful to count and store the named entities in a text\n",
    "\n",
    "# create an empty list\n",
    "named_ents1 = []\n",
    "\n",
    "# go through the entities and append each to the list\n",
    "for ent in doc1.ents:\n",
    "    named_ents1.append((ent.text, ent.label_))\n",
    "    \n",
    "print(named_ents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4389991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>December 2024</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clean Energy Research Group</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CERG</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Canada</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SFU</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>daily</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Entity Label\n",
       "0                December 2024  DATE\n",
       "1  Clean Energy Research Group   ORG\n",
       "2                         CERG   ORG\n",
       "3                       Canada   GPE\n",
       "4                          SFU   ORG\n",
       "5                        daily  DATE"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a df for the entities, from the list above \n",
    "df_ents1 = pd.DataFrame(named_ents1)\n",
    "# name the columns\n",
    "df_ents1.columns = ['Entity', 'Label']\n",
    "# print\n",
    "df_ents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5dd48c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    December 2024\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Clean Energy Research Group\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CERG\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") published a paper calling for \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Canada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " to build “mass utility-scale solar mega projects,” according to an \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SFU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " news release. Utility-scale solar “refers to large solar installations designed to feed power directly onto the electric grid.” An electric grid is an “intricate system” that provides electricity “all the way from its generation to the customers that use it for their \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    daily\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " needs.”</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the entities\n",
    "displacy.render(doc1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e81ff",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have learned quite a bit! Text normalization includes:\n",
    "\n",
    "* Lowercasing the words\n",
    "* Tokenizing (identifying words, punctuation, anything else)\n",
    "* Stemming\n",
    "* Lemmatizing\n",
    "\n",
    "And we have learned to do this with both NLTK and spaCy.\n",
    "\n",
    "We have also learned about **Named Entity Recognition** and how to extract entities with spaCy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}