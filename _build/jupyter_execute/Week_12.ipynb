{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea3ac65",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288c01b",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "Topic modelling is a very useful technique to get information from a large dataset. It is a type of *unsupervised machine learning*. Recall that *supervised machine learning* involved learning patterns from data, given a dataset and labels (e.g., movie reviews and stars). In unsupervised machine learning, we find patterns, but we do not have labels associated with the data. The task is to learn to classify or cluster the data by exploiting the patterns or similarities in the documents.\n",
    "\n",
    "The basic idea behind topic modelling is that you can identify topics or themes in a collection of documents using words that co-occur. One of the implementations of this idea is LDA (Latent Dirichlet Allocation), which assumes a distribution that we try to find using words in the text. The figure below is from an easy to follow paper outlining LDA. \n",
    "\n",
    "<figure>\n",
    "    <img src=\"./img/Blei_topic_modelling.png\" style=\"width:450px;\"\n",
    "         alt=\"supervised classification\">\n",
    "</figure>\n",
    "\n",
    "Diagram from: Blei, D. M. (2012). [Probabilistic topic models](https://www.cs.columbia.edu/~blei/papers/Blei2012.pdf). Communications of the ACM, 55(4), 77-84. \n",
    "\n",
    "The data for topic modelling needs to be normalized following the usual steps we have done so far: tokenization, lemmatization, and stopword removal. Then, we use a topic modelling module to find the topics in the data. \n",
    "\n",
    "The implementation here is based on a project on extracting topics from news articles, [TACT](https://github.com/sfu-discourse-lab/TACT/tree/master). \n",
    "\n",
    "If you want to see topic models in action, go to the [research site](https://gendergaptracker.research.sfu.ca/apps/topicmodel) for the Gender Gap Tracker and check topics for any month in the last 8 years. Those are the main topics covered in Canadian news. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb9b61",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will work with a collection of news articles, a part of the [SFU Opinion and Comments Corpus (SOCC)](https://github.com/sfu-discourse-lab/SOCC). The corpus was collected in our lab, the Discourse Processing Lab, for a project on evaluative language in online news comments. It consists of: opinion articles, comments, and annotated comments from the Canadian newspaper _The Globe and Mail_. We'll work with the articles, which should be in the data directory. If not, you can always download the corpus directly from the page above or from its [Kaggle page](https://www.kaggle.com/datasets/mtaboada/sfu-opinion-and-comments-corpus-socc) and save the `gnm_articles.csv` file to your data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e976a4e",
   "metadata": {},
   "source": [
    "## Install gensim and import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04585265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/yifangyuan/.local/lib/python3.11/site-packages (4.3.3)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/yifangyuan/.local/lib/python3.11/site-packages (from gensim) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/yifangyuan/.local/lib/python3.11/site-packages (from gensim) (1.13.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "# run only once\n",
    "!pip install gensim --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3256e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141047cc",
   "metadata": {},
   "source": [
    "## Import and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9fe6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/gnm_articles.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af0b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10339 entries, 0 to 10338\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   article_id           10339 non-null  int64  \n",
      " 1   title                10339 non-null  object \n",
      " 2   article_url          10339 non-null  object \n",
      " 3   author               10339 non-null  object \n",
      " 4   published_date       10339 non-null  object \n",
      " 5   ncomments            10339 non-null  float64\n",
      " 6   ntop_level_comments  10339 non-null  float64\n",
      " 7   article_text         10339 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 646.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57417dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>article_url</th>\n",
       "      <th>author</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ncomments</th>\n",
       "      <th>ntop_level_comments</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26842506</td>\n",
       "      <td>The Tories deserve another mandate - Stephen H...</td>\n",
       "      <td>http://www.theglobeandmail.com/opinion/editori...</td>\n",
       "      <td>GLOBE EDITORIAL</td>\n",
       "      <td>2015-10-16 EDT</td>\n",
       "      <td>2187.0</td>\n",
       "      <td>1378.0</td>\n",
       "      <td>&lt;p&gt;All elections are choices among imperfect a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26055892</td>\n",
       "      <td>Harper hysteria a sign of closed liberal minds</td>\n",
       "      <td>http://www.theglobeandmail.com/opinion/harper-...</td>\n",
       "      <td>Konrad Yakabuski</td>\n",
       "      <td>2015-08-24 EDT</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>&lt;p&gt;If even a fraction of the darkness that his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6929035</td>\n",
       "      <td>Too many first nations people live in a dream ...</td>\n",
       "      <td>http://www.theglobeandmail.com/opinion/too-man...</td>\n",
       "      <td>Jeffrey Simpson</td>\n",
       "      <td>2013-01-05 EST</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>&lt;p&gt;Large elements of aboriginal Canada live in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19047636</td>\n",
       "      <td>The Globe's editorial board endorses Tim Hudak...</td>\n",
       "      <td>http://www.theglobeandmail.com/opinion/editori...</td>\n",
       "      <td>GLOBE EDITORIAL</td>\n",
       "      <td>2014-06-06 EDT</td>\n",
       "      <td>905.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>&lt;p&gt;Over four days, The Globe editorial board l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11672346</td>\n",
       "      <td>Disgruntled Arab states look to strip Canada o...</td>\n",
       "      <td>http://www.theglobeandmail.com/news/world/disg...</td>\n",
       "      <td>Campbell Clark</td>\n",
       "      <td>2013-05-02 EDT</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>&lt;p&gt;Growing discontent among Arab nations over ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0    26842506  The Tories deserve another mandate - Stephen H...   \n",
       "1    26055892     Harper hysteria a sign of closed liberal minds   \n",
       "2     6929035  Too many first nations people live in a dream ...   \n",
       "3    19047636  The Globe's editorial board endorses Tim Hudak...   \n",
       "4    11672346  Disgruntled Arab states look to strip Canada o...   \n",
       "\n",
       "                                         article_url            author  \\\n",
       "0  http://www.theglobeandmail.com/opinion/editori...   GLOBE EDITORIAL   \n",
       "1  http://www.theglobeandmail.com/opinion/harper-...  Konrad Yakabuski   \n",
       "2  http://www.theglobeandmail.com/opinion/too-man...   Jeffrey Simpson   \n",
       "3  http://www.theglobeandmail.com/opinion/editori...   GLOBE EDITORIAL   \n",
       "4  http://www.theglobeandmail.com/news/world/disg...    Campbell Clark   \n",
       "\n",
       "   published_date  ncomments  ntop_level_comments  \\\n",
       "0  2015-10-16 EDT     2187.0               1378.0   \n",
       "1  2015-08-24 EDT     1103.0                455.0   \n",
       "2  2013-01-05 EST     1164.0                433.0   \n",
       "3  2014-06-06 EDT      905.0                432.0   \n",
       "4  2013-05-02 EDT     1129.0                411.0   \n",
       "\n",
       "                                        article_text  \n",
       "0  <p>All elections are choices among imperfect a...  \n",
       "1  <p>If even a fraction of the darkness that his...  \n",
       "2  <p>Large elements of aboriginal Canada live in...  \n",
       "3  <p>Over four days, The Globe editorial board l...  \n",
       "4  <p>Growing discontent among Arab nations over ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0d487",
   "metadata": {},
   "source": [
    "## Define a function to process the article_text column\n",
    "\n",
    "The only information we are interested in is in the `article_text` column, which contains the body of the articles. We will reuse and modify the function from last week to clean up that data. We will:\n",
    "\n",
    "* Remove the html tokens (the text has `<p>` and `</p>` to mark paragraphs)\n",
    "* Tokenize and lowercase\n",
    "* Remove stopwords\n",
    "* Lemmatize -- this one is added from last week; we use the WordNet lemmatizer\n",
    "\n",
    "We apply this function to the original df, to create a new column, `article_processed`. Then, we create a dictionary from that blob of text. You can also first extract the `article_text` to a string and process it, then create the dictionary (like we did in Week 12). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360f2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that: 1. tokenizes, 2. lowercases and\n",
    "# 3. lemmatizes\n",
    "\n",
    "# create the lemmatizer first\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define the function\n",
    "def process_text(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    words = word_tokenize(clean_text.lower())\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef7e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/wz/znms9m4s22s58bm3g8k29fd40000gn/T/ipykernel_11621/1945498291.py\", line 1, in <module>\n",
      "    df['article_processed'] = df['article_text'].apply(process_text)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py\", line 4764, in apply\n",
      "    ).apply()\n",
      "      ^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 1209, in apply\n",
      "    return self.apply_standard()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 1289, in apply_standard\n",
      "    mapped = obj._map_values(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py\", line 921, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py\", line 1814, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"lib.pyx\", line 2926, in pandas._libs.lib.map_infer\n",
      "  File \"/var/folders/wz/znms9m4s22s58bm3g8k29fd40000gn/T/ipykernel_11621/3407638260.py\", line 12, in process_text\n",
      "    lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_words]\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/wz/znms9m4s22s58bm3g8k29fd40000gn/T/ipykernel_11621/3407638260.py\", line 12, in <listcomp>\n",
      "    lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_words]\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/nltk/stem/wordnet.py\", line 85, in lemmatize\n",
      "    lemmas = self._morphy(word, pos)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/nltk/stem/wordnet.py\", line 41, in _morphy\n",
      "    return wn._morphy(form, pos, check_exceptions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py\", line 2109, in _morphy\n",
      "    return filter_forms([form] + forms)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py\", line None, in filter_forms\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "df['article_processed'] = df['article_text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484a49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645de09",
   "metadata": {},
   "source": [
    "## Parameters for gensim\n",
    "\n",
    "Now that we have clean text, we will process it with gensim functions. The first one creates a dictionary of the words in the text. The second creates a corpus, a bag of words with the frequency of all the words. Finally, the LdaModel actually creates the topic model.\n",
    "\n",
    "There are several parameters you can set with LDA models. For instance, you may want to filter out words that have very few instances, because they are only representative of one or two articles. In this case, what we are setting as parameters are the number of topics and the number of iterations. We train the model with 10 topics, that is, we assume that there are 10 topics across the entire dataset. And we do 15 passes. \n",
    "\n",
    "Once we have extracted the topics, we can inspect the most representative X words in each topic. In this case, I set it to 20. One thing you could do is to try and label the topics. So, for instance, these are the two top topics when I run the model:\n",
    "\n",
    "* Topic 0: party, liberal, conservative, election, government, harper, minister, political, would, leader, ndp, campaign, prime, new, one, vote, voter, trudeau, could, time\n",
    "\n",
    "* Topic 1: per, cent, government, year, tax, canada, would, health, care, money, system, budget, cost, canadian, one, rate, spending, education, province, country\n",
    "\n",
    "I could rename them to: \n",
    "\n",
    "* Topic 0 = Canadian politics\n",
    "* Topic 1 = Budget and government spending\n",
    "\n",
    "The labels in the [Gender Gap Tracker](https://gendergaptracker.research.sfu.ca/apps/topicmodel) site are done manually every month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gensim function to create a dictionary of the words in the text\n",
    "dictionary = corpora.Dictionary(df['article_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can inspect the contents of that dictionary\n",
    "for token, token_id in list(dictionary.token2id.items())[:10]:\n",
    "    print('{} => {}'.format(token, token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the doc2bow function to create a corpus, a bag of words (bow) of the text and the word counts\n",
    "corpus = [dictionary.doc2bow(text) for text in df['article_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d518e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top 20 words for each topic\n",
    "topics = lda_model.print_topics(num_words=20)\n",
    "for topic_num, topic in topics:\n",
    "    print(f\"Topic {topic_num}: \", end=\"\")\n",
    "    words = topic.split(' + ')\n",
    "    word_list = [word.split('*')[1].strip('\\\"') for word in words] \n",
    "    print(\", \".join(word_list))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also print the weight of each word in the topic\n",
    "for topic_num, topic in topics:\n",
    "    print(\"Topic {}:\".format(topic_num), end=\" \")\n",
    "    words = topic.split(' + ')\n",
    "    word_weight_list = [\"{} ({:.4f})\".format(term.strip('\\\"'), float(weight)) for weight, term in (word.split('*') for word in words)]\n",
    "    print(\", \".join(word_weight_list))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba3bab",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have learned about topic modelling and how to extract topics from data using the LDA model in the gensim library. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}