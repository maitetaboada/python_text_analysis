{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d49d90",
   "metadata": {},
   "source": [
    "# Ling 380 - Week 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea452ee",
   "metadata": {},
   "source": [
    "# Processing survey data\n",
    "\n",
    "Survey answers are typically of the style below, with a very specific question and a [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) that forces the answers into clearly defined categories.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./img/Likert.png\" style=\"width:450px;\"\n",
    "         alt=\"supervised classification\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "This is easy to process, as the answers can be tabulated. Some surveys, however, allow free-text answers. This is the case in the type of question you get at the end of a survey, something like \"Anything else you'd like to tell us?\". It's also common in focus groups and submissions in public consultation processes. These can go from one-word answers and short sentences to long paragraphs. That's the type of unstructured data that you need text analysis for!\n",
    "\n",
    "There are many ways we can analyze and summarize the information. we'll focus on:\n",
    "\n",
    "* Visualization with word clouds\n",
    "* NER recognition\n",
    "* Redaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228754f",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "There are many public surveys and sources of data (see at the end for more links). Here, we will work with the [Democracy Checkup](https://odesi.ca/en/details?id=/odesi/doi__10-5683_SP3_TEKM3T.xml) distributed by [Odesi](https://odesi.ca/en), a Canadian consortium that holds social science data. This is a survey of Canadian attitudes about democratic values, public policies, and current issues: \n",
    "\n",
    "* Harell, Allison; Stephenson, B. Laura; Rubenson, Daniel; Loewen, Peter John, 2023, \"Democracy Checkup, 2022. Canada\", https://doi.org/10.5683/SP3/TEKM3T, Borealis, V1, UNF:6:ufqbMikbXcaHqVhbaEXR3w== (fileUNF)\n",
    "\n",
    "The data contains many different variables, most of them numeric or on a scale with fixed values to choose from. But some of the values are free-form text answers; we'll study those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb84a30",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57138738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/anaconda3/lib/python3.11/site-packages (1.9.4)\r\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/yifangyuan/.local/lib/python3.11/site-packages (from wordcloud) (1.26.4)\r\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (10.2.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (3.8.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.25.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "# run this only once, to install wordcloud\n",
    "\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e111d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RegistryError",
     "evalue": "[E892] Unknown function registry: 'vectors'.\n\nAvailable names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRegistryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     55\u001b[0m         name,\n\u001b[1;32m     56\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     57\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     58\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     59\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     60\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:432\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:468\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload(vocab\u001b[38;5;241m=\u001b[39mvocab, disable\u001b[38;5;241m=\u001b[39mdisable, enable\u001b[38;5;241m=\u001b[39menable, exclude\u001b[38;5;241m=\u001b[39mexclude, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_init_py(\u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:649\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    650\u001b[0m     data_path,\n\u001b[1;32m    651\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    652\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    653\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    654\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    655\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    656\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    657\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:506\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    504\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config)\n\u001b[1;32m    505\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[0;32m--> 506\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    507\u001b[0m     config,\n\u001b[1;32m    508\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    509\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    510\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    511\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    512\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    513\u001b[0m )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:554\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[1;32m    553\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 554\u001b[0m nlp \u001b[38;5;241m=\u001b[39m lang_cls\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[1;32m    555\u001b[0m     config,\n\u001b[1;32m    556\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    557\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    558\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    559\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    560\u001b[0m     auto_fill\u001b[38;5;241m=\u001b[39mauto_fill,\n\u001b[1;32m    561\u001b[0m     validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    562\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/language.py:1764\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[0;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     filled[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m orig_pretraining\n\u001b[1;32m   1763\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m orig_pretraining\n\u001b[0;32m-> 1764\u001b[0m resolved_nlp \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mresolve(\n\u001b[1;32m   1765\u001b[0m     filled[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m], validate\u001b[38;5;241m=\u001b[39mvalidate, schema\u001b[38;5;241m=\u001b[39mConfigSchemaNlp\n\u001b[1;32m   1766\u001b[0m )\n\u001b[1;32m   1767\u001b[0m create_tokenizer \u001b[38;5;241m=\u001b[39m resolved_nlp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1768\u001b[0m before_creation \u001b[38;5;241m=\u001b[39m resolved_nlp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore_creation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/confection/__init__.py:760\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[0;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresolve\u001b[39m(\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m     validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    759\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 760\u001b[0m     resolved, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_make(\n\u001b[1;32m    761\u001b[0m         config, schema\u001b[38;5;241m=\u001b[39mschema, overrides\u001b[38;5;241m=\u001b[39moverrides, validate\u001b[38;5;241m=\u001b[39mvalidate, resolve\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     )\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/confection/__init__.py:809\u001b[0m, in \u001b[0;36mregistry._make\u001b[0;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interpolated:\n\u001b[1;32m    808\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config(orig_config)\u001b[38;5;241m.\u001b[39minterpolate()\n\u001b[0;32m--> 809\u001b[0m filled, _, resolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fill(\n\u001b[1;32m    810\u001b[0m     config, schema, validate\u001b[38;5;241m=\u001b[39mvalidate, overrides\u001b[38;5;241m=\u001b[39moverrides, resolve\u001b[38;5;241m=\u001b[39mresolve\n\u001b[1;32m    811\u001b[0m )\n\u001b[1;32m    812\u001b[0m filled \u001b[38;5;241m=\u001b[39m Config(filled, section_order\u001b[38;5;241m=\u001b[39msection_order)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/confection/__init__.py:863\u001b[0m, in \u001b[0;36mregistry._fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    861\u001b[0m     field \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39m__fields__[key]\n\u001b[1;32m    862\u001b[0m     schema\u001b[38;5;241m.\u001b[39m__fields__[key] \u001b[38;5;241m=\u001b[39m copy_model_field(field, Any)\n\u001b[0;32m--> 863\u001b[0m promise_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmake_promise_schema(value, resolve\u001b[38;5;241m=\u001b[39mresolve)\n\u001b[1;32m    864\u001b[0m filled[key], validation[v_key], final[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fill(\n\u001b[1;32m    865\u001b[0m     value,\n\u001b[1;32m    866\u001b[0m     promise_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m     overrides\u001b[38;5;241m=\u001b[39moverrides,\n\u001b[1;32m    871\u001b[0m )\n\u001b[1;32m    872\u001b[0m reg_name, func_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_constructor(final[key])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/confection/__init__.py:1069\u001b[0m, in \u001b[0;36mregistry.make_promise_schema\u001b[0;34m(cls, obj, resolve)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resolve \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mhas(reg_name, func_name):\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EmptySchema\n\u001b[0;32m-> 1069\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget(reg_name, func_name)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# Read the argument annotations and defaults from the function signature\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m id_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:127\u001b[0m, in \u001b[0;36mregistry.get\u001b[0;34m(cls, registry_name, func_name)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, registry_name):\n\u001b[1;32m    126\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_registry_names()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RegistryError(Errors\u001b[38;5;241m.\u001b[39mE892\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mregistry_name, available\u001b[38;5;241m=\u001b[39mnames))\n\u001b[1;32m    128\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, registry_name)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRegistryError\u001b[0m: [E892] Unknown function registry: 'vectors'.\n\nAvailable names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df975a0d",
   "metadata": {},
   "source": [
    "## Import and examine the data\n",
    "\n",
    "The data is huge: 9,829 rows (answers) and 501 questions (columns). As you can see from the output of `df.head()`, most of the data is numerical, so we'll work only with text columns. Luckily, those columns always end with the string \"TEXT\", so we can use pandas to extract them. For instance: \n",
    "\n",
    "* dc22_vote_choice_6_TEXT -- what party the person intends to vote for (in Dec 2022)\n",
    "* dc22_soc_media_use_9_TEXT -- what social media the person uses\n",
    "* dc22_language_3_TEXT -- what language the person speaks\n",
    "* etc.\n",
    "\n",
    "You can use the [Data explorer](https://borealisdata.ca/data-explorer/?siteUrl=https:%2F%2Fborealisdata.ca&dvLocale=en&fileId=659110) for this dataset to check the actual questions for each of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cora-cdem-2022_F1.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e919ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f163a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns contain the word \"TEXT\" using a regular expression\n",
    "\n",
    "print(df.filter(regex='TEXT').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe, 'df_text' that contains only those columns\n",
    "\n",
    "df_text = df.filter(regex='TEXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1e6ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feb50b",
   "metadata": {},
   "source": [
    "## Working with one column\n",
    "\n",
    "We will work with one of the columns, 'dc22_vote_choice_6_TEXT', where the question was: \"If a federal election were held today, which party would you be most likely to vote for?\" \n",
    "\n",
    "We will first extract data from that column to a pandas series. When extracting, we will drop any empty values (NaN) with `dropna()` and the columns that contain '-99', which means the person did not answer this question.\n",
    "\n",
    "Then, we put that into a string variable and lowercase it (so that \"People's Party\" and \"People's party\" are considered the same). But then we realize actually that the word \"party\" occurs here a lot, so we'll simply remove it with the stopwords. \n",
    "\n",
    "Finally, we have a somewhat clean string of words that we can send to WordCloud to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the values are '-99' (unanswered)\n",
    "# but you can sort other values to find the most frequent in a column\n",
    "\n",
    "df_text['dc22_vote_choice_6_TEXT'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e847846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice = df_text['dc22_vote_choice_6_TEXT'].dropna().loc[df_text['dc22_vote_choice_6_TEXT'].str.contains('99') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d926cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a pandas series, so we'll convert it to a string\n",
    "type(vote_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00268eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_str = ', '.join(vote_choice.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the contents of stop_words\n",
    "\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a63120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the word \"party\"\n",
    "\n",
    "stop_words.append('party')\n",
    "stop_words.append('Canada')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff78355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that: 1. tokenizes, 2. lowercases and\n",
    "# 3. removes stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_clean = clean_text(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fbf8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28606064",
   "metadata": {},
   "source": [
    "## Get frequencies\n",
    "\n",
    "You'll see below that WordCloud randomizes the font size of the output. But sometimes we want that to be meaningful, reflecting how frequent the word is. To get that information, we will use NLTK's FreqDist and generate a dictionary when we clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee990d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that: 1. tokenizes, 2. lowercases,\n",
    "# 3. removes stopwords and 4. counts the words\n",
    "\n",
    "def clean_text_freq(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    freq_dist = FreqDist(cleaned_words)\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_dict = clean_text_freq(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307165f",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "Word clouds are popular and can look very cool in a report. They can also be misleading if the size of the font does not correspond to the frequency of the word in the data, so you should use them with caution.  \n",
    "\n",
    "We'll use the [Wordcloud](https://github.com/amueller/word_cloud/) library, which we imported above. There are many options for how to do this. Here are two possibilities, changing the shape and the background colour. Check out the [example gallery](https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c63291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "# this actually generates the word cloud\n",
    "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n",
    "wc.generate(vote_choice_clean)\n",
    "\n",
    "# and this displays it\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d179694",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(vote_choice_clean)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad759f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigger\n",
    "wordcloud = WordCloud().generate(vote_choice_clean)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ff8d5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## With the frequency dictionary\n",
    "Recall that above we created a dictionary of the frequency of each word. We can use it to display relative to frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04817ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate_from_frequencies(vote_choice_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53606d5",
   "metadata": {},
   "source": [
    "# Named entities\n",
    "\n",
    "Another thing you may want to do with survey results is extract the named entities mentioned in the text. Remember that we can do this with spaCy (see Week6). \n",
    "\n",
    "We will use the variable `vote_choice_str` from earlier, which is simply the running text of the column about vote choice. We process it with spaCy (which was imported at the top) and we can print the entities that are of type 'ORG', which should correspond to a political party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb10b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_doc = nlp(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all entities and their label\n",
    "# note that there are errors here\n",
    "\n",
    "for ent in vote_choice_doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaab60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ORG entities\n",
    "\n",
    "for ent in vote_choice_doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55130640",
   "metadata": {},
   "source": [
    "# Redacting documents\n",
    "\n",
    "This idea comes from an [NLP notebook on redacting names](https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/NLP_with_SpaCy/Automatic%20Redaction%20%20%26%20Sanitization%20of%20Document%20Using%20Spacy%20NER.ipynb). Once you have named entities identified (hopefully accurately), you can also use the NER output to redact any personal information. For instance, you can identify all the person's names and remove them or replace them with something like 'REDACTED'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentences = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.ent_type_ == 'PERSON':\n",
    "            redacted_sentences.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentences.append(token.text)\n",
    "    \n",
    "    return ' '.join(redacted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_redacted = sanitize_names(vote_choice_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_choice_redacted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ce09d",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have learned about processing and aggregating survey data. This notebook has used some concepts we have learned previously:\n",
    "\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Removing stopwords\n",
    "* Creating a function to clean text\n",
    "* Reading in and manipulating data in pandas\n",
    "\n",
    "New information:\n",
    "\n",
    "* Creating word clouds\n",
    "* Using NER (named entity recognition) to redact documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}